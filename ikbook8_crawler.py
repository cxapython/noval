#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ikbook8 å°è¯´çˆ¬è™« - åŸºäºé€šç”¨çˆ¬è™«æ¡†æ¶
è‡ªåŠ¨ç”Ÿæˆäºé…ç½®ç®¡ç†å™¨

è¿è¡Œè¦æ±‚ï¼š
- Python 3.8+
- ä¾èµ–é…ç½®æ–‡ä»¶: config_ikbook8.json
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼ˆå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•å°±æ˜¯é¡¹ç›®æ ¹ç›®å½•ï¼‰
sys.path.insert(0, str(Path(__file__).parent))

from loguru import logger
from backend.generic_crawler import GenericNovelCrawler


class Ikbook8Crawler:
    """
    ikbook8 ç½‘ç«™çˆ¬è™«
    åŸºäºé€šç”¨çˆ¬è™«æ¡†æ¶ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶: config_ikbook8.json
    """

    def __init__(self, book_id: str, max_workers: int = 5, use_proxy: bool = False):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param book_id: ä¹¦ç±ID
        :param max_workers: å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤5
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†ï¼Œé»˜è®¤False
        """
        # é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•çš„ configs ç›®å½•æŸ¥æ‰¾ï¼‰
        config_path = Path(__file__).parent / "configs" / "config_ikbook8.json"

        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        # åˆå§‹åŒ–é€šç”¨çˆ¬è™«
        self.crawler = GenericNovelCrawler(
            config_file=str(config_path),
            book_id=book_id,
            max_workers=max_workers,
            use_proxy=use_proxy
        )

        logger.info(f"ğŸš€ ikbook8 çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")

    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            logger.info("=" * 60)
            logger.info(f"ğŸ“š å¼€å§‹çˆ¬å– ikbook8 å°è¯´")
            logger.info("=" * 60)

            # æ‰§è¡Œçˆ¬å–
            self.crawler.run()

            logger.info("=" * 60)
            logger.info("âœ… çˆ¬å–å®Œæˆï¼")
            logger.info("=" * 60)

        except KeyboardInterrupt:
            logger.warning("âš ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            sys.exit(0)
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            raise


def main():
    try:
        crawler = Ikbook8Crawler(
            book_id="a105332573.html",
            max_workers=1,
            use_proxy=True
        )
        crawler.run()
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()

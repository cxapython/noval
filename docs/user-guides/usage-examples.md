# 📘 使用示例

完整的实战案例，从配置到爬取到阅读的全流程演示。

## 🎯 案例1：爬取小说网站

### 场景描述

爬取某小说网站的完整小说内容。

**目标网站结构**：
- 小说信息页：`https://www.example.com/book/12345`
- 章节列表页：`https://www.example.com/book/12345/chapters`
- 章节内容页：`https://www.example.com/chapter/xxxxx`

---

### 方式1：使用可视化选择器（推荐）⭐

#### 第1步：创建配置

1. 访问 http://localhost:3000/crawler
2. 点击"新建配置"
3. 填写基本信息：
   - 网站名称：示例小说网
   - 基础URL：https://www.example.com
   - 内容类型：小说

#### 第2步：配置小说信息页

1. 输入URL：`https://www.example.com/book/12345`
2. 点击"开始渲染"
3. 等待页面加载完成
4. 选择器自动打开

**选择标题**：
- 在页面中点击小说标题
- 查看生成的XPath候选
- 选择置信度最高的（例如：`//*[@class='book-title']`，98%）
- 字段类型选择"小说标题"
- 点击"确认添加"

**选择作者**：
- 点击作者名称
- 选择XPath（例如：`//*[@class='author-name']/text()`，95%）
- 字段类型选择"作者"
- 点击"确认添加"

**选择封面**：
- 点击封面图片
- 自动识别为图片元素，XPath包含`/@src`
- 字段类型选择"封面图片"
- 点击"确认添加"

**继续选择其他字段**：
- 简介
- 状态
- 分类

#### 第3步：配置章节列表页

1. 切换URL到章节列表页：`https://www.example.com/book/12345/chapters`
2. 点击"开始渲染"

**选择章节容器**：
- 点击任意一个章节项
- 按 `↑` 向上选择父元素，直到选中`<li>`元素
- XPath示例：`//ul[@class='chapter-list']/li`
- 匹配数：100（所有章节，正确！）
- 字段类型选择"章节容器"
- 点击"确认添加"

**配置相对路径**：
- 章节标题：`./a/text()`
- 章节链接：`./a/@href`
- 直接在表单中填写（无需再次点击）

#### 第4步：配置章节内容页

1. 切换URL到任意章节：`https://www.example.com/chapter/xxxxx`
2. 点击"开始渲染"

**选择章节标题**：
- 点击章节标题
- XPath示例：`//h1[@class='chapter-title']/text()`
- 点击"确认添加"

**选择正文内容**：
- 点击正文段落
- XPath示例：`//div[@class='content']//p`
- 匹配数：50+（所有段落，正确！）
- 点击"确认添加"

#### 第5步：批量导入和保存

1. 点击"批量导入字段"
2. 所有字段自动填充到配置表单
3. 检查配置无误
4. 点击"保存配置"
5. 配置保存成功！

#### 第6步：运行爬虫

**方式A：任务管理器**

1. 访问 http://localhost:3000/tasks
2. 选择刚才创建的配置
3. 输入小说ID：`12345`
4. 设置并发数：`5`
5. 点击"开始爬取"
6. 实时查看进度和日志

**方式B：生成独立脚本**

1. 返回配置编辑页面
2. 点击"生成爬虫"
3. 运行命令：
```bash
python3 example_crawler.py 12345 --workers 5
```

#### 第7步：查看结果

1. 访问 http://localhost:3000/reader
2. 在书架中找到刚才爬取的小说
3. 点击封面开始阅读
4. 选择喜欢的主题和字体
5. 享受阅读！

---

### 方式2：使用流程编辑器

#### 配置小说标题字段

**流程设计**：
```
XPath提取器 → 索引选择器(-1) → 去除空格 → 字符替换
```

**节点配置**：

1. **XPath提取器**
   - XPath：`//h1[@class='book-title']/text()`

2. **索引选择器**
   - 索引：`-1`（取最后一个）

3. **去除空格**
   - 无需配置

4. **字符替换**
   - 查找：`书名：`
   - 替换：``（空字符串）

5. 点击"生成配置"并应用

#### 配置章节内容字段

**流程设计**：
```
XPath提取器 → 索引选择器(999) → 正则替换 → 合并数组
```

**节点配置**：

1. **XPath提取器**
   - XPath：`//div[@class='content']//p/text()`

2. **索引选择器**
   - 索引：`999`（保留所有段落）

3. **正则替换**
   - 正则：`.*广告.*`
   - 替换：``
   - 标志：`g`

4. **合并数组**
   - 分隔符：`\n`

5. 生成配置并保存

---

## 🎯 案例2：爬取新闻网站

### 场景描述

爬取某科技新闻网站的文章。

**目标**：
- 新闻列表页：提取所有新闻链接
- 新闻详情页：提取标题、作者、时间、正文

### 配置步骤

#### 1. 新建配置

- 网站名称：科技新闻网
- 内容类型：**新闻**
- 基础URL：https://tech.example.com

#### 2. 配置新闻列表页

使用可视化选择器：

**新闻容器**：
```xpath
//div[@class='news-list']/article
```

**新闻标题**（相对路径）：
```xpath
./h2/a/text()
```

**新闻链接**（相对路径）：
```xpath
./h2/a/@href
```

#### 3. 配置新闻内容页

**标题**：
```xpath
//h1[@class='news-title']/text()
```

**作者**：
```xpath
//span[@class='author']/text()
```

**发布时间**：
```xpath
//time[@class='publish-time']/@datetime
```

**正文内容**：
```xpath
//div[@class='article-content']//p/text()
```
后处理：索引选择器(999) → 合并数组(\n)

#### 4. 运行爬取

```bash
# 爬取ID为2025001的新闻
python3 tech_news_crawler.py 2025001
```

---

## 🎯 案例3：处理复杂页面

### 场景描述

目标网站有以下特点：
- 动态加载内容
- 需要翻页获取完整章节列表
- 正文中混杂广告

### 解决方案

#### 1. 处理动态内容

使用可视化选择器（自动支持JavaScript渲染）：
- 元素选择器通过Playwright加载
- 自动等待动态内容加载
- 选择渲染后的元素

#### 2. 配置分页

在章节列表配置中添加分页：

**下一页翻页**：
```json
{
  "pagination": {
    "type": "next_page",
    "next_page_xpath": "//a[@class='next-page']/@href",
    "max_pages": 10
  }
}
```

**URL模板分页**：
```json
{
  "pagination": {
    "type": "url_template",
    "url_template": "https://www.example.com/chapters?page={page}",
    "start_page": 1,
    "max_pages": 50
  }
}
```

#### 3. 过滤广告内容

使用流程编辑器添加正则替换：

**正则表达式**：
```regex
.*广告.*|.*推广.*|.*本站提示.*
```

**替换为**：
```
（空字符串）
```

完整流程：
```
XPath提取器 → 索引选择器(999) → 正则替换 → 合并数组
```

---

## 🎯 案例4：批量爬取多个小说

### 场景描述

需要爬取同一网站的多个小说。

### 方法1：命令行批量

创建脚本 `batch_crawl.sh`：

```bash
#!/bin/bash

# 小说ID列表
novel_ids=(12345 12346 12347 12348 12349)

# 循环爬取
for id in "${novel_ids[@]}"
do
  echo "开始爬取小说: $id"
  python3 example_crawler.py "$id" --workers 5
  echo "完成爬取小说: $id"
  echo "等待5秒..."
  sleep 5
done

echo "所有小说爬取完成！"
```

运行：
```bash
chmod +x batch_crawl.sh
./batch_crawl.sh
```

### 方法2：任务管理器队列

1. 在任务管理器中创建多个任务
2. 设置不同的小说ID
3. 任务会自动排队执行
4. 实时监控所有任务进度

---

## 🎯 案例5：数据清洗和替换

### 场景描述

爬取的小说内容包含大量广告和格式问题。

### 解决方案

#### 方法1：配置时添加后处理

在流程编辑器中添加清洗规则：

```
XPath提取器 
  → 索引选择器(999)
  → 正则替换（去除广告）
  → 正则替换（清理空白）
  → 合并数组
```

**正则规则**：

去除广告：
```regex
pattern: .*(?:广告|推广|本站提示|最新章节).*
replace: 
```

清理多余空白：
```regex
pattern: \s+
replace: 
```

#### 方法2：爬取后批量替换

使用阅读器的文字替换功能：

1. 打开任意章节
2. 点击"文字替换"
3. 输入替换规则：
   - 查找：`广告内容`
   - 替换：``（空）
4. 点击"预览"查看效果
5. 确认后"应用到所有章节"

#### 方法3：正则批量替换

勾选"使用正则表达式"：

**去除章节结束标语**：
```regex
查找: 本章完.*$
替换: 
```

**统一省略号**：
```regex
查找: \.{2,}
替换: ……
```

**去除作者唠叨**：
```regex
查找: \(作者的话:.*?\)
替换: 
```

---

## 💡 最佳实践总结

### 配置阶段

1. **优先使用可视化选择器**
   - 效率最高
   - 准确率最高
   - 学习成本最低

2. **测试多个样本**
   - 测试不同的小说/文章
   - 确保配置通用性
   - 处理边缘情况

3. **合理使用后处理**
   - 配置阶段清洗好数据
   - 减少后期手动处理
   - 保持配置简洁

### 爬取阶段

1. **选择合适并发数**
   - 先从低并发测试
   - 观察网站反应
   - 逐步提高到最优值

2. **使用代理池**
   - 高频爬取建议使用
   - 降低被封风险
   - 提高成功率

3. **实时监控日志**
   - 关注错误信息
   - 及时调整策略
   - 记录问题章节

### 阅读阶段

1. **选择合适主题**
   - 白天用默认或护眼主题
   - 夜间用夜间主题
   - 长时间阅读用护眼主题

2. **使用文字替换**
   - 清理广告内容
   - 统一格式
   - 修正错别字

3. **善用书签功能**
   - 标记精彩片段
   - 记录阅读心得
   - 快速定位内容

---

## 📚 更多资源

- [可视化元素选择器](../features/visual-selector.md) - 详细使用指南
- [流程编辑器](../features/flow-editor.md) - 高级数据处理
- [任务管理](../features/task-manager.md) - 任务执行和监控
- [小说阅读器](../features/novel-reader.md) - 阅读功能说明

---

**返回**: [文档中心](../README.md) | **下一篇**: [快捷键参考](shortcuts.md)


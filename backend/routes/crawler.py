#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«é…ç½®ç®¡ç†è·¯ç”±
"""
import json
import os
import requests
import base64
from pathlib import Path
from flask import Blueprint, request, jsonify
from loguru import logger
from playwright.sync_api import sync_playwright

crawler_bp = Blueprint('crawler', __name__)

CONFIG_DIR = Path(__file__).parent.parent.parent / 'configs'
CONFIG_PATTERN = "config_*.json"
# çˆ¬è™«æ–‡ä»¶ç›´æ¥ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œæ–¹ä¾¿è¿è¡Œ
CRAWLER_DIR = Path(__file__).parent.parent.parent


@crawler_bp.route('/configs', methods=['GET'])
def list_configs():
    """è·å–æ‰€æœ‰é…ç½®æ–‡ä»¶åˆ—è¡¨"""
    try:
        configs = []
        for config_file in CONFIG_DIR.glob(CONFIG_PATTERN):
            if config_file.name == 'config_template.json':
                continue
            
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    configs.append({
                        'filename': config_file.name,
                        'name': data.get('site_info', {}).get('name', 'Unknown'),
                        'description': data.get('site_info', {}).get('description', ''),
                        'base_url': data.get('site_info', {}).get('base_url', ''),
                    })
            except Exception as e:
                logger.warning(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ {config_file.name}: {e}")
        
        return jsonify({'success': True, 'configs': configs})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/config/<filename>', methods=['GET'])
def get_config(filename):
    """è·å–æŒ‡å®šé…ç½®æ–‡ä»¶å†…å®¹"""
    try:
        if not filename.startswith('config_') or not filename.endswith('.json'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„é…ç½®æ–‡ä»¶å'}), 400
        
        config_path = CONFIG_DIR / filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        return jsonify({'success': True, 'config': config_data})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/config', methods=['POST'])
def create_config():
    """åˆ›å»ºæ–°é…ç½®æ–‡ä»¶"""
    try:
        data = request.json
        logger.info(f"æ¥æ”¶åˆ°åˆ›å»ºé…ç½®è¯·æ±‚: {data}")
        
        site_name = data.get('site_name', '').strip()
        
        if not site_name:
            logger.warning("åˆ›å»ºé…ç½®å¤±è´¥: ç½‘ç«™åç§°ä¸ºç©º")
            return jsonify({'success': False, 'error': 'ç½‘ç«™åç§°ä¸èƒ½ä¸ºç©º'}), 400
        
        filename = f"config_{site_name}.json"
        config_path = CONFIG_DIR / filename
        
        logger.info(f"å°†åˆ›å»ºé…ç½®æ–‡ä»¶: {config_path}")
        
        if config_path.exists():
            logger.warning(f"åˆ›å»ºé…ç½®å¤±è´¥: æ–‡ä»¶å·²å­˜åœ¨ {filename}")
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶å·²å­˜åœ¨'}), 400
        
        template_path = CONFIG_DIR / 'config_template.json'
        with open(template_path, 'r', encoding='utf-8') as f:
            template = json.load(f)
        
        if 'config' in data:
            config_content = data['config']
            logger.info("ä½¿ç”¨è¯·æ±‚ä¸­æä¾›çš„é…ç½®å†…å®¹")
        else:
            config_content = template
            config_content['site_info']['name'] = site_name
            logger.info("ä½¿ç”¨æ¨¡æ¿åˆ›å»ºé…ç½®å†…å®¹")
        
        # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(config_path), exist_ok=True)
        
        try:
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config_content, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… é…ç½®æ–‡ä»¶åˆ›å»ºæˆåŠŸ: {filename}")
        except Exception as write_error:
            logger.error(f"âŒ å†™å…¥é…ç½®æ–‡ä»¶å¤±è´¥: {write_error}")
            return jsonify({'success': False, 'error': f'å†™å…¥æ–‡ä»¶å¤±è´¥: {str(write_error)}'}), 500
        
        return jsonify({'success': True, 'filename': filename})
    
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/config/<filename>', methods=['PUT'])
def update_config(filename):
    """æ›´æ–°é…ç½®æ–‡ä»¶"""
    try:
        if not filename.startswith('config_') or not filename.endswith('.json'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„é…ç½®æ–‡ä»¶å'}), 400
        
        config_path = CONFIG_DIR / filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        data = request.json
        config_content = data.get('config')
        
        if not config_content:
            return jsonify({'success': False, 'error': 'é…ç½®å†…å®¹ä¸èƒ½ä¸ºç©º'}), 400
        
        try:
            json.dumps(config_content)
        except:
            return jsonify({'success': False, 'error': 'JSONæ ¼å¼é”™è¯¯'}), 400
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_content, f, ensure_ascii=False, indent=2)
        
        return jsonify({'success': True, 'message': 'é…ç½®å·²æ›´æ–°'})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/config/<filename>', methods=['DELETE'])
def delete_config(filename):
    """åˆ é™¤é…ç½®æ–‡ä»¶"""
    try:
        if not filename.startswith('config_') or not filename.endswith('.json'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„é…ç½®æ–‡ä»¶å'}), 400
        
        if filename == 'config_template.json':
            return jsonify({'success': False, 'error': 'ä¸èƒ½åˆ é™¤æ¨¡æ¿æ–‡ä»¶'}), 400
        
        config_path = CONFIG_DIR / filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        os.remove(config_path)
        
        return jsonify({'success': True, 'message': 'é…ç½®å·²åˆ é™¤'})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/template', methods=['GET'])
def get_template():
    """è·å–é…ç½®æ¨¡æ¿"""
    try:
        template_path = CONFIG_DIR / 'config_template.json'
        with open(template_path, 'r', encoding='utf-8') as f:
            template = json.load(f)
        
        return jsonify({'success': True, 'template': template})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/validate', methods=['POST'])
def validate_config():
    """éªŒè¯é…ç½®æ–‡ä»¶æ ¼å¼"""
    try:
        data = request.json
        config = data.get('config')
        
        if not config:
            return jsonify({'success': False, 'error': 'é…ç½®å†…å®¹ä¸èƒ½ä¸ºç©º'})
        
        errors = []
        
        if 'site_info' not in config:
            errors.append('ç¼ºå°‘ site_info å­—æ®µ')
        else:
            if 'name' not in config['site_info']:
                errors.append('ç¼ºå°‘ site_info.name å­—æ®µ')
            if 'base_url' not in config['site_info']:
                errors.append('ç¼ºå°‘ site_info.base_url å­—æ®µ')
        
        if 'parsers' not in config:
            errors.append('ç¼ºå°‘ parsers å­—æ®µ')
        else:
            if 'novel_info' not in config['parsers']:
                errors.append('ç¼ºå°‘ parsers.novel_info å­—æ®µ')
            if 'chapter_list' not in config['parsers']:
                errors.append('ç¼ºå°‘ parsers.chapter_list å­—æ®µ')
            if 'chapter_content' not in config['parsers']:
                errors.append('ç¼ºå°‘ parsers.chapter_content å­—æ®µ')
        
        if errors:
            return jsonify({'success': False, 'valid': False, 'errors': errors})
        
        return jsonify({'success': True, 'valid': True, 'message': 'é…ç½®æ ¼å¼æ­£ç¡®'})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/generate-crawler/<filename>', methods=['POST'])
def generate_crawler(filename):
    """ç”Ÿæˆä¸“ç”¨çˆ¬è™«ä»£ç ï¼ˆä¸ä¿å­˜ï¼Œè¿”å›ä»£ç ä¾›ç¼–è¾‘ï¼‰"""
    try:
        if not filename.startswith('config_') or not filename.endswith('.json'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„é…ç½®æ–‡ä»¶å'}), 400
        
        config_path = CONFIG_DIR / filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        site_name = config.get('site_info', {}).get('name', 'unknown')
        
        crawler_content = generate_crawler_code(site_name, filename)
        crawler_filename = f"{site_name}_crawler.py"
        
        logger.info(f"ğŸ“ ç”Ÿæˆçˆ¬è™«ä»£ç : {crawler_filename}")
        
        return jsonify({
            'success': True, 
            'message': f'çˆ¬è™«ä»£ç å·²ç”Ÿæˆ',
            'filename': crawler_filename,
            'path': crawler_filename,
            'content': crawler_content
        })
    
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆçˆ¬è™«ä»£ç å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/save-crawler', methods=['POST'])
def save_crawler():
    """ä¿å­˜çˆ¬è™«ä»£ç åˆ°æ–‡ä»¶"""
    try:
        data = request.json
        filename = data.get('filename', '').strip()
        content = data.get('content', '').strip()
        
        if not filename or not content:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶åå’Œå†…å®¹ä¸èƒ½ä¸ºç©º'}), 400
        
        # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸ä¿å­˜åˆ°çˆ¬è™«ç›®å½•
        if not filename.endswith('_crawler.py'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶å'}), 400
        
        # ä¿å­˜æ–‡ä»¶åˆ°é¡¹ç›®æ ¹ç›®å½•
        crawler_path = CRAWLER_DIR / filename
        with open(crawler_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"âœ… ä¿å­˜çˆ¬è™«æ–‡ä»¶: {filename}")
        
        return jsonify({
            'success': True, 
            'message': f'çˆ¬è™«æ–‡ä»¶å·²ä¿å­˜: {filename}',
            'path': filename
        })
    
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜çˆ¬è™«æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/test-parser', methods=['POST'])
def test_parser():
    """æµ‹è¯•å•ä¸ªè§£æå™¨é…ç½®"""
    try:
        data = request.json
        url = data.get('url', '').strip()
        parser_config = data.get('parser_config', {})
        request_config = data.get('request_config', {})
        
        if not url:
            return jsonify({'success': False, 'error': 'URLä¸èƒ½ä¸ºç©º'}), 400
        
        if not parser_config:
            return jsonify({'success': False, 'error': 'è§£æå™¨é…ç½®ä¸èƒ½ä¸ºç©º'}), 400
        
        # è·å–é¡µé¢å†…å®¹
        headers = request_config.get('headers', {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15'
        })
        timeout = request_config.get('timeout', 30)
        encoding = request_config.get('encoding')
        
        try:
            response = requests.get(url, headers=headers, timeout=timeout, verify=False)
            
            if encoding:
                response.encoding = encoding
            else:
                response.encoding = response.apparent_encoding or 'utf-8'
            
            html = response.text
            
            if response.status_code != 200:
                return jsonify({
                    'success': False,
                    'error': f'HTTPçŠ¶æ€ç : {response.status_code}'
                }), 400
                
        except Exception as e:
            return jsonify({
                'success': False,
                'error': f'è·å–é¡µé¢å¤±è´¥: {str(e)}'
            }), 400
        
        # ä½¿ç”¨é€šç”¨çˆ¬è™«çš„è§£ææ–¹æ³•
        from backend.generic_crawler import GenericNovelCrawler
        
        # åˆ›å»ºä¸´æ—¶çˆ¬è™«å®ä¾‹æ¥ä½¿ç”¨è§£ææ–¹æ³•
        temp_config = {
            'site_info': {'name': 'test', 'base_url': url},
            'url_templates': {
                'book_detail': '/book/{book_id}',
                'chapter_list_page': '/book/{book_id}/{page}/',
                'chapter_content_page': '/book/{book_id}/{chapter_id}_{page}.html'
            },
            'parsers': {}
        }
        
        # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
            json.dump(temp_config, f)
            temp_config_file = f.name
        
        try:
            crawler = GenericNovelCrawler(
                config_file=temp_config_file,
                book_id='test',
                max_workers=1
            )
            
            # è§£æ
            result = crawler._parse_with_config(html, parser_config)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_config_file)
            
            # å¤„ç†ç»“æœ
            result_preview = result
            if isinstance(result, list):
                if len(result) > 10:
                    result_preview = result[:10]
                    result_info = f"åˆ—è¡¨ç±»å‹ï¼Œå…±{len(result)}é¡¹ï¼Œæ˜¾ç¤ºå‰10é¡¹"
                else:
                    result_info = f"åˆ—è¡¨ç±»å‹ï¼Œå…±{len(result)}é¡¹"
            elif isinstance(result, str):
                result_info = f"å­—ç¬¦ä¸²ç±»å‹ï¼Œé•¿åº¦{len(result)}"
                if len(result) > 500:
                    result_preview = result[:500] + '...'
            else:
                result_info = f"ç±»å‹: {type(result).__name__}"
            
            return jsonify({
                'success': True,
                'result': result_preview,
                'result_info': result_info,
                'html_preview': html[:1000] if len(html) > 1000 else html,
                'html_length': len(html)
            })
            
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_config_file):
                os.remove(temp_config_file)
            raise e
    
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è§£æå™¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/render-page', methods=['POST'])
def render_page():
    """æ¸²æŸ“é¡µé¢å¹¶è¿”å›æˆªå›¾å’ŒHTML"""
    try:
        data = request.json
        url = data.get('url', '').strip()
        
        if not url:
            return jsonify({'success': False, 'error': 'URLä¸èƒ½ä¸ºç©º'}), 400
        
        logger.info(f"ğŸ“¸ å¼€å§‹æ¸²æŸ“é¡µé¢: {url}")
        
        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = p.chromium.launch(headless=True)
            page = browser.new_page(
                viewport={'width': 1280, 'height': 1024},
                user_agent='Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15'
            )
            
            try:
                # è®¿é—®é¡µé¢
                page.goto(url, wait_until='networkidle', timeout=30000)
                
                # è·å–é¡µé¢HTML
                html = page.content()
                
                # è·å–æˆªå›¾
                screenshot_bytes = page.screenshot(full_page=True)
                screenshot_base64 = base64.b64encode(screenshot_bytes).decode('utf-8')
                
                # è·å–é¡µé¢æ ‡é¢˜
                title = page.title()
                
                browser.close()
                
                logger.info(f"âœ… é¡µé¢æ¸²æŸ“æˆåŠŸ: {title}")
                
                return jsonify({
                    'success': True,
                    'title': title,
                    'html': html[:50000],  # é™åˆ¶HTMLå¤§å°
                    'html_length': len(html),
                    'screenshot': f'data:image/png;base64,{screenshot_base64}'
                })
                
            except Exception as e:
                browser.close()
                raise e
                
    except Exception as e:
        logger.error(f"âŒ æ¸²æŸ“é¡µé¢å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


def selector_to_xpath(css_selector):
    """ç®€å•çš„CSSé€‰æ‹©å™¨è½¬XPathï¼ˆåŸºç¡€å®ç°ï¼‰"""
    if not css_selector:
        return '//*'
    # ç®€å•è½¬æ¢ï¼Œå¤„ç†å¸¸è§æƒ…å†µ
    xpath = css_selector
    xpath = xpath.replace('>', '/')
    xpath = xpath.replace(' ', '//')
    if not xpath.startswith('//'):
        xpath = '//' + xpath
    return xpath


@crawler_bp.route('/generate-xpath', methods=['POST'])
def generate_xpath():
    """æ ¹æ®CSSé€‰æ‹©å™¨æˆ–å…ƒç´ ä¿¡æ¯ç”ŸæˆXPath"""
    try:
        data = request.json
        url = data.get('url', '').strip()
        selector = data.get('selector', '').strip()  # CSSé€‰æ‹©å™¨
        element_text = data.get('element_text', '').strip()  # å…ƒç´ æ–‡æœ¬å†…å®¹
        
        if not url:
            return jsonify({'success': False, 'error': 'URLä¸èƒ½ä¸ºç©º'}), 400
        
        logger.info(f"ğŸ” ç”ŸæˆXPath: URL={url}, Selector={selector}, Text={element_text}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                page.goto(url, wait_until='networkidle', timeout=30000)
                
                # è·å–å…ƒç´ å¹¶ç”Ÿæˆå¤šç§XPathå»ºè®®
                xpath_suggestions = []
                
                # æ–¹æ³•1ï¼šåŸºäºCSSé€‰æ‹©å™¨è½¬æ¢
                if selector:
                    try:
                        element = page.query_selector(selector)
                        if element:
                            # è·å–å…ƒç´ ä¿¡æ¯
                            tag_name = element.evaluate('el => el.tagName.toLowerCase()')
                            class_name = element.get_attribute('class')
                            id_attr = element.get_attribute('id')
                            
                            # è·å–çˆ¶å…ƒç´ å’Œå…„å¼Ÿå…ƒç´ ä¿¡æ¯ï¼Œç”Ÿæˆæ›´é€šç”¨çš„ç»“æ„åŒ–XPath
                            parent_tag = element.evaluate('el => el.parentElement?.tagName?.toLowerCase() || ""')
                            parent_class = element.evaluate('el => el.parentElement?.className || ""')
                            
                            # ç”Ÿæˆå¤šç§XPathï¼ˆä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–è·¯å¾„ï¼Œé¿å…å…·ä½“æ–‡æœ¬ï¼‰
                            
                            # ä¼˜å…ˆçº§1: åŸºäºIDï¼ˆæœ€ç¨³å®šï¼‰
                            if id_attr:
                                xpath_suggestions.append({
                                    'xpath': f'//{tag_name}[@id="{id_attr}"]',
                                    'type': 'âœ… IDé€‰æ‹©å™¨ï¼ˆæ¨èï¼‰',
                                    'description': 'åŸºäºå”¯ä¸€IDï¼Œæœ€ç¨³å®š',
                                    'priority': 1
                                })
                            
                            # ä¼˜å…ˆçº§2: åŸºäºå®Œæ•´class
                            if class_name:
                                classes = class_name.strip().split()
                                if classes:
                                    class_xpath = f'//{tag_name}[@class="{class_name}"]'
                                    xpath_suggestions.append({
                                        'xpath': class_xpath,
                                        'type': 'âš¡ å®Œæ•´Classï¼ˆç²¾ç¡®ï¼‰',
                                        'description': f'åŒ¹é…å®Œæ•´classå±æ€§',
                                        'priority': 2
                                    })
                                    
                                    # ä¼˜å…ˆçº§3: åŸºäºå•ä¸ªclassï¼ˆæ›´é€šç”¨ï¼‰
                                    for cls in classes[:3]:  # æœ€å¤šå‰3ä¸ªclass
                                        xpath_suggestions.append({
                                            'xpath': f'//{tag_name}[contains(@class, "{cls}")]',
                                            'type': f'ğŸ¯ å•ä¸ªClass: {cls}',
                                            'description': f'åŒ¹é…åŒ…å«è¯¥classçš„å…ƒç´ ',
                                            'priority': 3
                                        })
                            
                            # ä¼˜å…ˆçº§4: åŸºäºçˆ¶å…ƒç´ ç»“æ„ï¼ˆé€šç”¨ï¼‰
                            if parent_tag and parent_class:
                                parent_classes = parent_class.strip().split()
                                if parent_classes:
                                    # çˆ¶å…ƒç´ class + å­å…ƒç´ tag
                                    xpath_suggestions.append({
                                        'xpath': f'//{parent_tag}[contains(@class, "{parent_classes[0]}")]//{tag_name}',
                                        'type': f'ğŸ—ï¸ ç»“æ„è·¯å¾„ï¼ˆé€šç”¨ï¼‰',
                                        'description': f'ä»çˆ¶å…ƒç´ å‘ä¸‹æŸ¥æ‰¾',
                                        'priority': 4
                                    })
                            
                            # ä¼˜å…ˆçº§5: åŸºäºæ ‡ç­¾åçš„ä½ç½®ç´¢å¼•
                            # è®¡ç®—åŒçº§åŒç±»å‹å…ƒç´ çš„ä½ç½®
                            try:
                                position = element.evaluate('''
                                    el => {
                                        let pos = 1;
                                        let prev = el.previousElementSibling;
                                        while (prev) {
                                            if (prev.tagName === el.tagName) pos++;
                                            prev = prev.previousElementSibling;
                                        }
                                        return pos;
                                    }
                                ''')
                                if position > 1:
                                    xpath_suggestions.append({
                                        'xpath': f'({selector_to_xpath(selector)})[{position}]',
                                        'type': f'ğŸ“ ä½ç½®ç´¢å¼•',
                                        'description': f'ç¬¬{position}ä¸ªåŒç±»å…ƒç´ ',
                                        'priority': 5
                                    })
                            except:
                                pass
                            
                            # ä¼˜å…ˆçº§6: çº¯æ ‡ç­¾åï¼ˆæœ€é€šç”¨ï¼Œä½†å¯èƒ½åŒ¹é…å¤šä¸ªï¼‰
                            xpath_suggestions.append({
                                'xpath': f'//{tag_name}',
                                'type': 'âš ï¸ æ ‡ç­¾åï¼ˆå¯èƒ½ä¸ç²¾ç¡®ï¼‰',
                                'description': 'åŒ¹é…æ‰€æœ‰è¯¥æ ‡ç­¾ï¼Œå¯èƒ½éœ€è¦æŒ‡å®šindex',
                                'priority': 6
                            })
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸  CSSé€‰æ‹©å™¨å¤„ç†å¤±è´¥: {e}")
                
                # æ–¹æ³•2ï¼šåŸºäºå…ƒç´ æ–‡æœ¬æœç´¢ï¼ˆä»…ä½œä¸ºå‚è€ƒï¼Œä¸æ¨èï¼‰
                if element_text and len(xpath_suggestions) == 0:
                    xpath_suggestions.append({
                        'xpath': f'//*[contains(text(), "{element_text[:20]}")]',
                        'type': 'âš ï¸ æ–‡æœ¬æœç´¢ï¼ˆä¸æ¨èï¼‰',
                        'description': 'åŸºäºæ–‡æœ¬å†…å®¹ï¼Œæ¢æ–‡ç« ä¼šå¤±æ•ˆ',
                        'priority': 10
                    })
                
                # æŒ‰ä¼˜å…ˆçº§æ’åº
                xpath_suggestions.sort(key=lambda x: x['priority'])
                
                browser.close()
                
                if not xpath_suggestions:
                    return jsonify({
                        'success': False,
                        'error': 'æœªèƒ½ç”ŸæˆXPathå»ºè®®ï¼Œè¯·æ£€æŸ¥é€‰æ‹©å™¨æ˜¯å¦æ­£ç¡®'
                    }), 400
                
                logger.info(f"âœ… ç”Ÿæˆäº† {len(xpath_suggestions)} ä¸ªXPathå»ºè®®")
                
                return jsonify({
                    'success': True,
                    'suggestions': xpath_suggestions
                })
                
            except Exception as e:
                browser.close()
                raise e
                
    except Exception as e:
        logger.error(f"âŒ ç”ŸæˆXPathå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/test-config', methods=['POST'])
def test_config():
    """æµ‹è¯•å®Œæ•´é…ç½®"""
    try:
        data = request.json
        url = data.get('url', '').strip()
        config = data.get('config', {})
        test_type = data.get('test_type', 'novel_info')  # novel_info, chapter_list, chapter_content
        
        if not url:
            return jsonify({'success': False, 'error': 'URLä¸èƒ½ä¸ºç©º'}), 400
        
        if not config:
            return jsonify({'success': False, 'error': 'é…ç½®ä¸èƒ½ä¸ºç©º'}), 400
        
        # éªŒè¯é…ç½®åŸºæœ¬ç»“æ„
        required_top_level = ['site_info', 'parsers']
        missing = [f for f in required_top_level if f not in config]
        if missing:
            return jsonify({
                'success': False,
                'error': f'é…ç½®ç¼ºå°‘å¿…éœ€çš„é¡¶å±‚å­—æ®µ: {", ".join(missing)}'
            }), 400
        
        # éªŒè¯ site_info
        if 'base_url' not in config.get('site_info', {}):
            return jsonify({
                'success': False,
                'error': 'site_info ä¸­ç¼ºå°‘ base_url å­—æ®µ'
            }), 400
        
        # ä¸ºæµ‹è¯•æ·»åŠ é»˜è®¤çš„ url_templatesï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if 'url_templates' not in config:
            config['url_templates'] = {
                'book_detail': '/book/{book_id}',
                'chapter_list_page': '/book/{book_id}/{page}/',
                'chapter_content_page': '/book/{book_id}/{chapter_id}_{page}.html'
            }
        
        # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
            json.dump(config, f)
            temp_config_file = f.name
        
        try:
            from backend.generic_crawler_debug import GenericNovelCrawlerDebug
            
            # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä½¿ç”¨è°ƒè¯•ç‰ˆæœ¬ï¼‰
            crawler = GenericNovelCrawlerDebug(
                config_file=temp_config_file,
                book_id='test',
                max_workers=1
            )
            
            # è·å–é¡µé¢
            html = crawler.get_page(url)
            if not html:
                os.remove(temp_config_file)
                return jsonify({
                    'success': False,
                    'error': 'è·å–é¡µé¢å¤±è´¥'
                }), 400
            
            results = {}
            
            if test_type == 'novel_info':
                # æµ‹è¯•å°è¯´ä¿¡æ¯è§£æï¼ˆä½¿ç”¨è°ƒè¯•æ¨¡å¼ï¼‰
                result = crawler.parse_novel_info_debug(html)
                results = {
                    'type': 'å°è¯´ä¿¡æ¯',
                    'data': result['data'],
                    'debug': result['debug']
                }
            
            elif test_type == 'chapter_list':
                # æµ‹è¯•ç« èŠ‚åˆ—è¡¨è§£æ
                chapter_list_config = config.get('parsers', {}).get('chapter_list', {})
                
                # ç±»å‹æ£€æŸ¥
                if not isinstance(chapter_list_config, dict):
                    os.remove(temp_config_file)
                    return jsonify({
                        'success': False,
                        'error': f'chapter_listé…ç½®æ ¼å¼é”™è¯¯ï¼šåº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º{type(chapter_list_config).__name__}'
                    }), 400
                
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['items', 'title', 'url']
                missing_fields = [f for f in required_fields if f not in chapter_list_config]
                if missing_fields:
                    os.remove(temp_config_file)
                    return jsonify({
                        'success': False,
                        'error': f'chapter_listé…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µï¼š{", ".join(missing_fields)}'
                    }), 400
                
                chapters = crawler._parse_chapters_from_page(html, chapter_list_config)
                
                results = {
                    'type': 'ç« èŠ‚åˆ—è¡¨',
                    'total': len(chapters),
                    'sample': chapters[:5] if len(chapters) > 5 else chapters,
                    'info': f'å…±è§£æå‡º{len(chapters)}ä¸ªç« èŠ‚ï¼Œæ˜¾ç¤ºå‰5ä¸ª'
                }
            
            elif test_type == 'chapter_content':
                # æµ‹è¯•ç« èŠ‚å†…å®¹è§£æï¼ˆä½¿ç”¨è°ƒè¯•æ¨¡å¼ï¼‰
                result = crawler.download_chapter_content_debug(url)
                content = result['content']
                
                results = {
                    'type': 'ç« èŠ‚å†…å®¹',
                    'length': len(content),
                    'full_content': content,  # è¿”å›å®Œæ•´å†…å®¹
                    'debug': result['debug'],
                    'info': f'å†…å®¹é•¿åº¦: {len(content)}å­—'
                }
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_config_file)
            
            return jsonify({
                'success': True,
                'results': results
            })
            
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_config_file):
                os.remove(temp_config_file)
            raise e
    
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•é…ç½®å¤±è´¥: {e}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@crawler_bp.route('/run-crawler', methods=['POST'])
def run_crawler():
    """è¿è¡Œçˆ¬è™«ï¼ˆé€šè¿‡ä»»åŠ¡ç®¡ç†å™¨ï¼‰"""
    try:
        data = request.json
        config_filename = data.get('config_filename', '').strip()
        book_id = data.get('book_id', '').strip()
        start_url = data.get('start_url', '').strip()
        max_workers = data.get('max_workers', 5)
        use_proxy = data.get('use_proxy', False)
        
        if not config_filename:
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶åä¸èƒ½ä¸ºç©º'}), 400
        
        if not book_id and not start_url:
            return jsonify({'success': False, 'error': 'è¯·æä¾›ä¹¦ç±IDæˆ–å®Œæ•´URL'}), 400
        
        config_path = CONFIG_DIR / config_filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # å¦‚æœæä¾›äº†å®Œæ•´URLï¼Œä»URLä¸­æå–book_id
        if start_url and not book_id:
            # å°è¯•ä»URLä¸­æå–IDï¼ˆå‡è®¾æ ¼å¼ä¸º /book/12345.html æˆ–ç±»ä¼¼ï¼‰
            import re
            match = re.search(r'/(\d+)', start_url)
            if match:
                book_id = match.group(1)
            else:
                return jsonify({'success': False, 'error': 'æ— æ³•ä»URLä¸­æå–ä¹¦ç±ID'}), 400
        
        # åˆ›å»ºä»»åŠ¡
        task_id = task_manager.create_task(
            config_filename=config_filename,
            book_id=book_id,
            max_workers=max_workers,
            use_proxy=use_proxy
        )
        
        # è·å–socketioå®ä¾‹
        socketio = get_socketio()
        
        # çˆ¬è™«å·¥å‚å‡½æ•°
        def crawler_factory(task_obj):
            def progress_callback(**kwargs):
                """è¿›åº¦å›è°ƒ"""
                task_obj.update_progress(**kwargs)
                # é€šè¿‡WebSocketæ¨é€è¿›åº¦
                if socketio:
                    socketio.emit('task_progress', {
                        'task_id': task_obj.task_id,
                        'progress': task_obj.to_dict()
                    })
            
            def log_callback(level, message):
                """æ—¥å¿—å›è°ƒ"""
                task_obj.add_log(level, message)
                # é€šè¿‡WebSocketæ¨é€æ—¥å¿—
                if socketio:
                    socketio.emit('task_log', {
                        'task_id': task_obj.task_id,
                        'log': {
                            'level': level,
                            'message': message
                        }
                    })
            
            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            from backend.generic_crawler import GenericNovelCrawler
            crawler = GenericNovelCrawler(
                config_file=str(config_path),
                book_id=task_obj.book_id,
                max_workers=task_obj.max_workers,
                use_proxy=task_obj.use_proxy,
                progress_callback=progress_callback,
                log_callback=log_callback,
                stop_flag=task_obj.stop_flag
            )
            
            # åœ¨è§£æå®Œå°è¯´ä¿¡æ¯åæ›´æ–°ä»»åŠ¡ä¿¡æ¯
            original_parse_chapter_list = crawler.parse_chapter_list
            def wrapped_parse_chapter_list():
                result = original_parse_chapter_list()
                if result and crawler.novel_info:
                    task_obj.novel_title = crawler.novel_info.get('title', '')
                    task_obj.novel_author = crawler.novel_info.get('author', '')
                return result
            
            crawler.parse_chapter_list = wrapped_parse_chapter_list
            return crawler
        
        # å¯åŠ¨ä»»åŠ¡
        success = task_manager.start_task(task_id, crawler_factory)
        
        if success:
            return jsonify({
                'success': True,
                'task_id': task_id,
                'message': f'çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼ŒBook ID: {book_id}'
            })
        else:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡å¯åŠ¨å¤±è´¥'}), 500
        
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨çˆ¬è™«å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


def generate_crawler_code(site_name: str, config_file: str) -> str:
    """
    ç”Ÿæˆçˆ¬è™«ä»£ç 
    å…¼å®¹ Python 3.8+
    """
    template = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
{site_name} å°è¯´çˆ¬è™« - åŸºäºé€šç”¨çˆ¬è™«æ¡†æ¶
è‡ªåŠ¨ç”Ÿæˆäºé…ç½®ç®¡ç†å™¨

è¿è¡Œè¦æ±‚ï¼š
- Python 3.8+
- ä¾èµ–é…ç½®æ–‡ä»¶: {config_file}
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼ˆå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•å°±æ˜¯é¡¹ç›®æ ¹ç›®å½•ï¼‰
sys.path.insert(0, str(Path(__file__).parent))

from loguru import logger
from backend.generic_crawler import GenericNovelCrawler


class {site_name.capitalize()}Crawler:
    """
    {site_name} ç½‘ç«™çˆ¬è™«
    åŸºäºé€šç”¨çˆ¬è™«æ¡†æ¶ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶: {config_file}
    """
    
    def __init__(self, book_id: str, max_workers: int = 5, use_proxy: bool = False):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param book_id: ä¹¦ç±ID
        :param max_workers: å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤5
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†ï¼Œé»˜è®¤False
        """
        # é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•çš„ configs ç›®å½•æŸ¥æ‰¾ï¼‰
        config_path = Path(__file__).parent / "configs" / "{config_file}"
        
        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {{config_path}}")
        
        # åˆå§‹åŒ–é€šç”¨çˆ¬è™«
        self.crawler = GenericNovelCrawler(
            config_file=str(config_path),
            book_id=book_id,
            max_workers=max_workers,
            use_proxy=use_proxy
        )
        
        logger.info(f"ğŸš€ {site_name} çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            logger.info("=" * 60)
            logger.info(f"ğŸ“š å¼€å§‹çˆ¬å– {site_name} å°è¯´")
            logger.info("=" * 60)
            
            # æ‰§è¡Œçˆ¬å–
            self.crawler.run()
            
            logger.info("=" * 60)
            logger.info("âœ… çˆ¬å–å®Œæˆï¼")
            logger.info("=" * 60)
            
        except KeyboardInterrupt:
            logger.warning("âš ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            sys.exit(0)
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±è´¥: {{e}}")
            raise


def main():
    book_id = ""
    workers=5
    # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
    try:
        crawler = {site_name.capitalize()}Crawler(
            book_id=book_id,
            max_workers=workers,
            use_proxy=True
        )
        crawler.run()
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {{e}}")
        sys.exit(1)


if __name__ == '__main__':
    main()
'''
    return template


# ==================== ä»»åŠ¡ç®¡ç† API ====================

from backend.task_manager import task_manager, TaskStatus
from backend.generic_crawler import GenericNovelCrawler

def get_socketio():
    """å»¶è¿Ÿå¯¼å…¥socketioä»¥é¿å…å¾ªç¯ä¾èµ–"""
    try:
        from backend.api import socketio
        return socketio
    except ImportError:
        return None


@crawler_bp.route('/tasks', methods=['GET'])
def list_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
    try:
        tasks = task_manager.get_all_tasks()
        return jsonify({
            'success': True,
            'tasks': [task.to_dict() for task in tasks]
        })
    except Exception as e:
        logger.error(f"âŒ è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/task/<task_id>', methods=['GET'])
def get_task(task_id):
    """è·å–å•ä¸ªä»»åŠ¡è¯¦æƒ…"""
    try:
        task = task_manager.get_task(task_id)
        if not task:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
        
        return jsonify({
            'success': True,
            'task': task.to_dict()
        })
    except Exception as e:
        logger.error(f"âŒ è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/task/create', methods=['POST'])
def create_task():
    """åˆ›å»ºæ–°ä»»åŠ¡"""
    try:
        data = request.json
        config_filename = data.get('config_filename', '').strip()
        book_id = data.get('book_id', '').strip()
        start_url = data.get('start_url', '').strip()
        max_workers = data.get('max_workers', 5)
        use_proxy = data.get('use_proxy', False)
        
        if not config_filename:
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶åä¸èƒ½ä¸ºç©º'}), 400
        
        if not book_id and not start_url:
            return jsonify({'success': False, 'error': 'è¯·æä¾›ä¹¦ç±IDæˆ–å®Œæ•´URL'}), 400
        
        config_path = CONFIG_DIR / config_filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # å¦‚æœæä¾›äº†å®Œæ•´URLï¼Œä»URLä¸­æå–book_id
        if start_url and not book_id:
            import re
            match = re.search(r'/(\d+)', start_url)
            if match:
                book_id = match.group(1)
            else:
                return jsonify({'success': False, 'error': 'æ— æ³•ä»URLä¸­æå–ä¹¦ç±ID'}), 400
        
        # åˆ›å»ºä»»åŠ¡
        task_id = task_manager.create_task(
            config_filename=config_filename,
            book_id=book_id,
            max_workers=max_workers,
            use_proxy=use_proxy
        )
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'ä»»åŠ¡åˆ›å»ºæˆåŠŸ'
        })
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/task/<task_id>/start', methods=['POST'])
def start_task(task_id):
    """å¯åŠ¨ä»»åŠ¡"""
    try:
        task = task_manager.get_task(task_id)
        if not task:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
        
        config_path = CONFIG_DIR / task.config_filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # è·å–socketioå®ä¾‹
        socketio = get_socketio()
        
        # çˆ¬è™«å·¥å‚å‡½æ•°
        def crawler_factory(task_obj):
            def progress_callback(**kwargs):
                """è¿›åº¦å›è°ƒ"""
                task_obj.update_progress(**kwargs)
                # é€šè¿‡WebSocketæ¨é€è¿›åº¦
                if socketio:
                    socketio.emit('task_progress', {
                        'task_id': task_obj.task_id,
                        'progress': task_obj.to_dict()
                    })
            
            def log_callback(level, message):
                """æ—¥å¿—å›è°ƒ"""
                task_obj.add_log(level, message)
                # é€šè¿‡WebSocketæ¨é€æ—¥å¿—
                if socketio:
                    socketio.emit('task_log', {
                        'task_id': task_obj.task_id,
                        'log': {
                            'level': level,
                            'message': message
                        }
                    })
            
            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            crawler = GenericNovelCrawler(
                config_file=str(config_path),
                book_id=task_obj.book_id,
                max_workers=task_obj.max_workers,
                use_proxy=task_obj.use_proxy,
                progress_callback=progress_callback,
                log_callback=log_callback,
                stop_flag=task_obj.stop_flag
            )
            
            # åœ¨è§£æå®Œå°è¯´ä¿¡æ¯åæ›´æ–°ä»»åŠ¡ä¿¡æ¯
            original_parse_chapter_list = crawler.parse_chapter_list
            def wrapped_parse_chapter_list():
                result = original_parse_chapter_list()
                if result and crawler.novel_info:
                    task_obj.novel_title = crawler.novel_info.get('title', '')
                    task_obj.novel_author = crawler.novel_info.get('author', '')
                return result
            crawler.parse_chapter_list = wrapped_parse_chapter_list
            
            return crawler
        
        # å¯åŠ¨ä»»åŠ¡
        success = task_manager.start_task(task_id, crawler_factory)
        
        if success:
            # é€šè¿‡WebSocketé€šçŸ¥ä»»åŠ¡å¯åŠ¨
            if socketio:
                socketio.emit('task_started', {'task_id': task_id})
            
            return jsonify({
                'success': True,
                'message': f'ä»»åŠ¡å·²å¯åŠ¨: {task_id}'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡å¯åŠ¨å¤±è´¥'
            }), 400
        
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/task/<task_id>/stop', methods=['POST'])
def stop_task(task_id):
    """åœæ­¢ä»»åŠ¡"""
    try:
        success = task_manager.stop_task(task_id)
        
        if success:
            # é€šè¿‡WebSocketé€šçŸ¥ä»»åŠ¡åœæ­¢
            socketio = get_socketio()
            if socketio:
                socketio.emit('task_stopped', {'task_id': task_id})
            
            return jsonify({
                'success': True,
                'message': f'ä»»åŠ¡å·²åœæ­¢: {task_id}'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡åœæ­¢å¤±è´¥'
            }), 400
        
    except Exception as e:
        logger.error(f"âŒ åœæ­¢ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/task/<task_id>/delete', methods=['DELETE'])
def delete_task(task_id):
    """åˆ é™¤ä»»åŠ¡"""
    try:
        success = task_manager.delete_task(task_id)
        
        if success:
            return jsonify({
                'success': True,
                'message': f'ä»»åŠ¡å·²åˆ é™¤: {task_id}'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡åˆ é™¤å¤±è´¥'
            }), 400
        
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/task/<task_id>/logs', methods=['GET'])
def get_task_logs(task_id):
    """è·å–ä»»åŠ¡æ—¥å¿—"""
    try:
        limit = request.args.get('limit', 100, type=int)
        logs = task_manager.get_task_logs(task_id, limit)
        
        return jsonify({
            'success': True,
            'logs': logs
        })
        
    except Exception as e:
        logger.error(f"âŒ è·å–ä»»åŠ¡æ—¥å¿—å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/tasks/clear-completed', methods=['POST'])
def clear_completed_tasks():
    """æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡"""
    try:
        count = task_manager.clear_completed_tasks()
        return jsonify({
            'success': True,
            'message': f'å·²æ¸…ç† {count} ä¸ªä»»åŠ¡'
        })
    except Exception as e:
        logger.error(f"âŒ æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


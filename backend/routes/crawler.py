#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«é…ç½®ç®¡ç†è·¯ç”±
"""
import json
import os
import requests
import base64
from pathlib import Path
from flask import Blueprint, request, jsonify
from loguru import logger
from playwright.sync_api import sync_playwright

crawler_bp = Blueprint('crawler', __name__)

CONFIG_DIR = Path(__file__).parent.parent.parent / 'configs'
CONFIG_PATTERN = "config_*.json"
CRAWLER_DIR = Path(__file__).parent.parent.parent / 'crawler-manager' / 'backend' / 'crawlers'


@crawler_bp.route('/configs', methods=['GET'])
def list_configs():
    """è·å–æ‰€æœ‰é…ç½®æ–‡ä»¶åˆ—è¡¨"""
    try:
        configs = []
        for config_file in CONFIG_DIR.glob(CONFIG_PATTERN):
            if config_file.name == 'config_template.json':
                continue
            
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    configs.append({
                        'filename': config_file.name,
                        'name': data.get('site_info', {}).get('name', 'Unknown'),
                        'description': data.get('site_info', {}).get('description', ''),
                        'base_url': data.get('site_info', {}).get('base_url', ''),
                    })
            except Exception as e:
                logger.warning(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ {config_file.name}: {e}")
        
        return jsonify({'success': True, 'configs': configs})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/config/<filename>', methods=['GET'])
def get_config(filename):
    """è·å–æŒ‡å®šé…ç½®æ–‡ä»¶å†…å®¹"""
    try:
        if not filename.startswith('config_') or not filename.endswith('.json'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„é…ç½®æ–‡ä»¶å'}), 400
        
        config_path = CONFIG_DIR / filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        return jsonify({'success': True, 'config': config_data})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/config', methods=['POST'])
def create_config():
    """åˆ›å»ºæ–°é…ç½®æ–‡ä»¶"""
    try:
        data = request.json
        site_name = data.get('site_name', '').strip()
        
        if not site_name:
            return jsonify({'success': False, 'error': 'ç½‘ç«™åç§°ä¸èƒ½ä¸ºç©º'}), 400
        
        filename = f"config_{site_name}.json"
        config_path = CONFIG_DIR / filename
        
        if config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶å·²å­˜åœ¨'}), 400
        
        template_path = CONFIG_DIR / 'config_template.json'
        with open(template_path, 'r', encoding='utf-8') as f:
            template = json.load(f)
        
        if 'config' in data:
            config_content = data['config']
        else:
            config_content = template
            config_content['site_info']['name'] = site_name
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_content, f, ensure_ascii=False, indent=2)
        
        return jsonify({'success': True, 'filename': filename})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/config/<filename>', methods=['PUT'])
def update_config(filename):
    """æ›´æ–°é…ç½®æ–‡ä»¶"""
    try:
        if not filename.startswith('config_') or not filename.endswith('.json'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„é…ç½®æ–‡ä»¶å'}), 400
        
        config_path = CONFIG_DIR / filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        data = request.json
        config_content = data.get('config')
        
        if not config_content:
            return jsonify({'success': False, 'error': 'é…ç½®å†…å®¹ä¸èƒ½ä¸ºç©º'}), 400
        
        try:
            json.dumps(config_content)
        except:
            return jsonify({'success': False, 'error': 'JSONæ ¼å¼é”™è¯¯'}), 400
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_content, f, ensure_ascii=False, indent=2)
        
        return jsonify({'success': True, 'message': 'é…ç½®å·²æ›´æ–°'})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/config/<filename>', methods=['DELETE'])
def delete_config(filename):
    """åˆ é™¤é…ç½®æ–‡ä»¶"""
    try:
        if not filename.startswith('config_') or not filename.endswith('.json'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„é…ç½®æ–‡ä»¶å'}), 400
        
        if filename == 'config_template.json':
            return jsonify({'success': False, 'error': 'ä¸èƒ½åˆ é™¤æ¨¡æ¿æ–‡ä»¶'}), 400
        
        config_path = CONFIG_DIR / filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        os.remove(config_path)
        
        return jsonify({'success': True, 'message': 'é…ç½®å·²åˆ é™¤'})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/template', methods=['GET'])
def get_template():
    """è·å–é…ç½®æ¨¡æ¿"""
    try:
        template_path = CONFIG_DIR / 'config_template.json'
        with open(template_path, 'r', encoding='utf-8') as f:
            template = json.load(f)
        
        return jsonify({'success': True, 'template': template})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/validate', methods=['POST'])
def validate_config():
    """éªŒè¯é…ç½®æ–‡ä»¶æ ¼å¼"""
    try:
        data = request.json
        config = data.get('config')
        
        if not config:
            return jsonify({'success': False, 'error': 'é…ç½®å†…å®¹ä¸èƒ½ä¸ºç©º'})
        
        errors = []
        
        if 'site_info' not in config:
            errors.append('ç¼ºå°‘ site_info å­—æ®µ')
        else:
            if 'name' not in config['site_info']:
                errors.append('ç¼ºå°‘ site_info.name å­—æ®µ')
            if 'base_url' not in config['site_info']:
                errors.append('ç¼ºå°‘ site_info.base_url å­—æ®µ')
        
        if 'parsers' not in config:
            errors.append('ç¼ºå°‘ parsers å­—æ®µ')
        else:
            if 'novel_info' not in config['parsers']:
                errors.append('ç¼ºå°‘ parsers.novel_info å­—æ®µ')
            if 'chapter_list' not in config['parsers']:
                errors.append('ç¼ºå°‘ parsers.chapter_list å­—æ®µ')
            if 'chapter_content' not in config['parsers']:
                errors.append('ç¼ºå°‘ parsers.chapter_content å­—æ®µ')
        
        if errors:
            return jsonify({'success': False, 'valid': False, 'errors': errors})
        
        return jsonify({'success': True, 'valid': True, 'message': 'é…ç½®æ ¼å¼æ­£ç¡®'})
    
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/generate-crawler/<filename>', methods=['POST'])
def generate_crawler(filename):
    """ç”Ÿæˆä¸“ç”¨çˆ¬è™«ä»£ç ï¼ˆä¸ä¿å­˜ï¼Œè¿”å›ä»£ç ä¾›ç¼–è¾‘ï¼‰"""
    try:
        if not filename.startswith('config_') or not filename.endswith('.json'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„é…ç½®æ–‡ä»¶å'}), 400
        
        config_path = CONFIG_DIR / filename
        if not config_path.exists():
            return jsonify({'success': False, 'error': 'é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        site_name = config.get('site_info', {}).get('name', 'unknown')
        
        crawler_content = generate_crawler_code(site_name, filename)
        crawler_filename = f"{site_name}_crawler.py"
        relative_path = f"crawler-manager/backend/crawlers/{crawler_filename}"
        
        logger.info(f"ğŸ“ ç”Ÿæˆçˆ¬è™«ä»£ç : {crawler_filename}")
        
        return jsonify({
            'success': True, 
            'message': f'çˆ¬è™«ä»£ç å·²ç”Ÿæˆ',
            'filename': crawler_filename,
            'path': relative_path,
            'content': crawler_content
        })
    
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆçˆ¬è™«ä»£ç å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/save-crawler', methods=['POST'])
def save_crawler():
    """ä¿å­˜çˆ¬è™«ä»£ç åˆ°æ–‡ä»¶"""
    try:
        data = request.json
        filename = data.get('filename', '').strip()
        content = data.get('content', '').strip()
        
        if not filename or not content:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶åå’Œå†…å®¹ä¸èƒ½ä¸ºç©º'}), 400
        
        # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸ä¿å­˜åˆ°çˆ¬è™«ç›®å½•
        if not filename.endswith('_crawler.py'):
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶å'}), 400
        
        # ç¡®ä¿çˆ¬è™«ç›®å½•å­˜åœ¨
        CRAWLER_DIR.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        crawler_path = CRAWLER_DIR / filename
        with open(crawler_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        relative_path = f"crawler-manager/backend/crawlers/{filename}"
        logger.info(f"âœ… ä¿å­˜çˆ¬è™«æ–‡ä»¶: {relative_path}")
        
        return jsonify({
            'success': True, 
            'message': f'çˆ¬è™«æ–‡ä»¶å·²ä¿å­˜: {relative_path}',
            'path': relative_path
        })
    
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜çˆ¬è™«æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/test-parser', methods=['POST'])
def test_parser():
    """æµ‹è¯•å•ä¸ªè§£æå™¨é…ç½®"""
    try:
        data = request.json
        url = data.get('url', '').strip()
        parser_config = data.get('parser_config', {})
        request_config = data.get('request_config', {})
        
        if not url:
            return jsonify({'success': False, 'error': 'URLä¸èƒ½ä¸ºç©º'}), 400
        
        if not parser_config:
            return jsonify({'success': False, 'error': 'è§£æå™¨é…ç½®ä¸èƒ½ä¸ºç©º'}), 400
        
        # è·å–é¡µé¢å†…å®¹
        headers = request_config.get('headers', {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15'
        })
        timeout = request_config.get('timeout', 30)
        encoding = request_config.get('encoding')
        
        try:
            response = requests.get(url, headers=headers, timeout=timeout, verify=False)
            
            if encoding:
                response.encoding = encoding
            else:
                response.encoding = response.apparent_encoding or 'utf-8'
            
            html = response.text
            
            if response.status_code != 200:
                return jsonify({
                    'success': False,
                    'error': f'HTTPçŠ¶æ€ç : {response.status_code}'
                }), 400
                
        except Exception as e:
            return jsonify({
                'success': False,
                'error': f'è·å–é¡µé¢å¤±è´¥: {str(e)}'
            }), 400
        
        # ä½¿ç”¨é€šç”¨çˆ¬è™«çš„è§£ææ–¹æ³•
        from backend.generic_crawler import GenericNovelCrawler
        
        # åˆ›å»ºä¸´æ—¶çˆ¬è™«å®ä¾‹æ¥ä½¿ç”¨è§£ææ–¹æ³•
        temp_config = {
            'site_info': {'name': 'test', 'base_url': url},
            'url_patterns': {},
            'parsers': {}
        }
        
        # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
            json.dump(temp_config, f)
            temp_config_file = f.name
        
        try:
            crawler = GenericNovelCrawler(
                config_file=temp_config_file,
                book_id='test',
                max_workers=1
            )
            
            # è§£æ
            result = crawler._parse_with_config(html, parser_config)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_config_file)
            
            # å¤„ç†ç»“æœ
            result_preview = result
            if isinstance(result, list):
                if len(result) > 10:
                    result_preview = result[:10]
                    result_info = f"åˆ—è¡¨ç±»å‹ï¼Œå…±{len(result)}é¡¹ï¼Œæ˜¾ç¤ºå‰10é¡¹"
                else:
                    result_info = f"åˆ—è¡¨ç±»å‹ï¼Œå…±{len(result)}é¡¹"
            elif isinstance(result, str):
                result_info = f"å­—ç¬¦ä¸²ç±»å‹ï¼Œé•¿åº¦{len(result)}"
                if len(result) > 500:
                    result_preview = result[:500] + '...'
            else:
                result_info = f"ç±»å‹: {type(result).__name__}"
            
            return jsonify({
                'success': True,
                'result': result_preview,
                'result_info': result_info,
                'html_preview': html[:1000] if len(html) > 1000 else html,
                'html_length': len(html)
            })
            
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_config_file):
                os.remove(temp_config_file)
            raise e
    
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è§£æå™¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/render-page', methods=['POST'])
def render_page():
    """æ¸²æŸ“é¡µé¢å¹¶è¿”å›æˆªå›¾å’ŒHTML"""
    try:
        data = request.json
        url = data.get('url', '').strip()
        
        if not url:
            return jsonify({'success': False, 'error': 'URLä¸èƒ½ä¸ºç©º'}), 400
        
        logger.info(f"ğŸ“¸ å¼€å§‹æ¸²æŸ“é¡µé¢: {url}")
        
        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = p.chromium.launch(headless=True)
            page = browser.new_page(
                viewport={'width': 1280, 'height': 1024},
                user_agent='Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15'
            )
            
            try:
                # è®¿é—®é¡µé¢
                page.goto(url, wait_until='networkidle', timeout=30000)
                
                # è·å–é¡µé¢HTML
                html = page.content()
                
                # è·å–æˆªå›¾
                screenshot_bytes = page.screenshot(full_page=True)
                screenshot_base64 = base64.b64encode(screenshot_bytes).decode('utf-8')
                
                # è·å–é¡µé¢æ ‡é¢˜
                title = page.title()
                
                browser.close()
                
                logger.info(f"âœ… é¡µé¢æ¸²æŸ“æˆåŠŸ: {title}")
                
                return jsonify({
                    'success': True,
                    'title': title,
                    'html': html[:50000],  # é™åˆ¶HTMLå¤§å°
                    'html_length': len(html),
                    'screenshot': f'data:image/png;base64,{screenshot_base64}'
                })
                
            except Exception as e:
                browser.close()
                raise e
                
    except Exception as e:
        logger.error(f"âŒ æ¸²æŸ“é¡µé¢å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/generate-xpath', methods=['POST'])
def generate_xpath():
    """æ ¹æ®CSSé€‰æ‹©å™¨æˆ–å…ƒç´ ä¿¡æ¯ç”ŸæˆXPath"""
    try:
        data = request.json
        url = data.get('url', '').strip()
        selector = data.get('selector', '').strip()  # CSSé€‰æ‹©å™¨
        element_text = data.get('element_text', '').strip()  # å…ƒç´ æ–‡æœ¬å†…å®¹
        
        if not url:
            return jsonify({'success': False, 'error': 'URLä¸èƒ½ä¸ºç©º'}), 400
        
        logger.info(f"ğŸ” ç”ŸæˆXPath: URL={url}, Selector={selector}, Text={element_text}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            try:
                page.goto(url, wait_until='networkidle', timeout=30000)
                
                # è·å–å…ƒç´ å¹¶ç”Ÿæˆå¤šç§XPathå»ºè®®
                xpath_suggestions = []
                
                # æ–¹æ³•1ï¼šåŸºäºCSSé€‰æ‹©å™¨è½¬æ¢
                if selector:
                    try:
                        element = page.query_selector(selector)
                        if element:
                            # è·å–å…ƒç´ ä¿¡æ¯
                            tag_name = element.evaluate('el => el.tagName.toLowerCase()')
                            class_name = element.get_attribute('class')
                            id_attr = element.get_attribute('id')
                            text_content = element.text_content()
                            
                            # ç”Ÿæˆå¤šç§XPath
                            if id_attr:
                                xpath_suggestions.append({
                                    'xpath': f'//{tag_name}[@id="{id_attr}"]',
                                    'type': 'åŸºäºID',
                                    'priority': 1
                                })
                            
                            if class_name:
                                classes = class_name.strip().split()
                                if classes:
                                    class_xpath = f'//{tag_name}[@class="{class_name}"]'
                                    xpath_suggestions.append({
                                        'xpath': class_xpath,
                                        'type': 'åŸºäºå®Œæ•´class',
                                        'priority': 2
                                    })
                                    
                                    # åŸºäºå•ä¸ªclass
                                    for cls in classes[:2]:  # æœ€å¤šå‰ä¸¤ä¸ªclass
                                        xpath_suggestions.append({
                                            'xpath': f'//{tag_name}[contains(@class, "{cls}")]',
                                            'type': f'åŸºäºclass: {cls}',
                                            'priority': 3
                                        })
                            
                            if text_content and len(text_content.strip()) > 0:
                                text = text_content.strip()[:30]  # é™åˆ¶é•¿åº¦
                                xpath_suggestions.append({
                                    'xpath': f'//{tag_name}[contains(text(), "{text}")]',
                                    'type': 'åŸºäºæ–‡æœ¬å†…å®¹',
                                    'priority': 4
                                })
                    except Exception as e:
                        logger.warning(f"âš ï¸  CSSé€‰æ‹©å™¨å¤„ç†å¤±è´¥: {e}")
                
                # æ–¹æ³•2ï¼šåŸºäºæ–‡æœ¬å†…å®¹æœç´¢
                if element_text and len(xpath_suggestions) == 0:
                    xpath_suggestions.append({
                        'xpath': f'//*[contains(text(), "{element_text}")]',
                        'type': 'é€šç”¨æ–‡æœ¬æœç´¢',
                        'priority': 5
                    })
                
                # æŒ‰ä¼˜å…ˆçº§æ’åº
                xpath_suggestions.sort(key=lambda x: x['priority'])
                
                browser.close()
                
                if not xpath_suggestions:
                    return jsonify({
                        'success': False,
                        'error': 'æœªèƒ½ç”ŸæˆXPathå»ºè®®ï¼Œè¯·æ£€æŸ¥é€‰æ‹©å™¨æ˜¯å¦æ­£ç¡®'
                    }), 400
                
                logger.info(f"âœ… ç”Ÿæˆäº† {len(xpath_suggestions)} ä¸ªXPathå»ºè®®")
                
                return jsonify({
                    'success': True,
                    'suggestions': xpath_suggestions
                })
                
            except Exception as e:
                browser.close()
                raise e
                
    except Exception as e:
        logger.error(f"âŒ ç”ŸæˆXPathå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@crawler_bp.route('/test-config', methods=['POST'])
def test_config():
    """æµ‹è¯•å®Œæ•´é…ç½®"""
    try:
        data = request.json
        url = data.get('url', '').strip()
        config = data.get('config', {})
        test_type = data.get('test_type', 'novel_info')  # novel_info, chapter_list, chapter_content
        
        if not url:
            return jsonify({'success': False, 'error': 'URLä¸èƒ½ä¸ºç©º'}), 400
        
        if not config:
            return jsonify({'success': False, 'error': 'é…ç½®ä¸èƒ½ä¸ºç©º'}), 400
        
        # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
            json.dump(config, f)
            temp_config_file = f.name
        
        try:
            from backend.generic_crawler import GenericNovelCrawler
            
            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            crawler = GenericNovelCrawler(
                config_file=temp_config_file,
                book_id='test',
                max_workers=1
            )
            
            # è·å–é¡µé¢
            html = crawler.get_page(url)
            if not html:
                os.remove(temp_config_file)
                return jsonify({
                    'success': False,
                    'error': 'è·å–é¡µé¢å¤±è´¥'
                }), 400
            
            results = {}
            
            if test_type == 'novel_info':
                # æµ‹è¯•å°è¯´ä¿¡æ¯è§£æ
                novel_info = crawler.parse_novel_info(html)
                results = {
                    'type': 'å°è¯´ä¿¡æ¯',
                    'data': novel_info
                }
            
            elif test_type == 'chapter_list':
                # æµ‹è¯•ç« èŠ‚åˆ—è¡¨è§£æ
                chapter_list_config = config['parsers']['chapter_list']
                chapters = crawler._parse_chapters_from_page(html, chapter_list_config)
                
                results = {
                    'type': 'ç« èŠ‚åˆ—è¡¨',
                    'total': len(chapters),
                    'sample': chapters[:5] if len(chapters) > 5 else chapters,
                    'info': f'å…±è§£æå‡º{len(chapters)}ä¸ªç« èŠ‚ï¼Œæ˜¾ç¤ºå‰5ä¸ª'
                }
            
            elif test_type == 'chapter_content':
                # æµ‹è¯•ç« èŠ‚å†…å®¹è§£æ
                content = crawler.download_chapter_content(url)
                
                results = {
                    'type': 'ç« èŠ‚å†…å®¹',
                    'length': len(content),
                    'preview': content[:500] if len(content) > 500 else content,
                    'info': f'å†…å®¹é•¿åº¦: {len(content)}å­—ï¼Œæ˜¾ç¤ºå‰500å­—'
                }
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_config_file)
            
            return jsonify({
                'success': True,
                'results': results
            })
            
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_config_file):
                os.remove(temp_config_file)
            raise e
    
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•é…ç½®å¤±è´¥: {e}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


def generate_crawler_code(site_name: str, config_file: str) -> str:
    """
    ç”Ÿæˆçˆ¬è™«ä»£ç 
    å…¼å®¹ Python 3.8+
    """
    template = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
{site_name} å°è¯´çˆ¬è™« - åŸºäºé€šç”¨çˆ¬è™«æ¡†æ¶
è‡ªåŠ¨ç”Ÿæˆäºé…ç½®ç®¡ç†å™¨

è¿è¡Œè¦æ±‚ï¼š
- Python 3.8+
- ä¾èµ–é…ç½®æ–‡ä»¶: {config_file}
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from loguru import logger
from backend.generic_crawler import GenericNovelCrawler


class {site_name.capitalize()}Crawler:
    """
    {site_name} ç½‘ç«™çˆ¬è™«
    åŸºäºé€šç”¨çˆ¬è™«æ¡†æ¶ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶: {config_file}
    """
    
    def __init__(self, book_id: str, max_workers: int = 5, use_proxy: bool = False):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param book_id: ä¹¦ç±ID
        :param max_workers: å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤5
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†ï¼Œé»˜è®¤False
        """
        # é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•çš„ configs ç›®å½•æŸ¥æ‰¾ï¼‰
        config_path = Path(__file__).parent.parent.parent.parent / "configs" / "{config_file}"
        
        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {{config_path}}")
        
        # åˆå§‹åŒ–é€šç”¨çˆ¬è™«
        self.crawler = GenericNovelCrawler(
            config_file=str(config_path),
            book_id=book_id,
            max_workers=max_workers,
            use_proxy=use_proxy
        )
        
        logger.info(f"ğŸš€ {site_name} çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            logger.info("=" * 60)
            logger.info(f"ğŸ“š å¼€å§‹çˆ¬å– {site_name} å°è¯´")
            logger.info("=" * 60)
            
            # æ‰§è¡Œçˆ¬å–
            self.crawler.run()
            
            logger.info("=" * 60)
            logger.info("âœ… çˆ¬å–å®Œæˆï¼")
            logger.info("=" * 60)
            
        except KeyboardInterrupt:
            logger.warning("âš ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            sys.exit(0)
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±è´¥: {{e}}")
            raise


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description=f'{site_name} å°è¯´çˆ¬è™«',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
  python crawler-manager/backend/crawlers/{site_name}_crawler.py 12345
  
  # ä½¿ç”¨ä»£ç†
  python crawler-manager/backend/crawlers/{site_name}_crawler.py 12345 --proxy
  
  # æŒ‡å®šå¹¶å‘æ•°
  python crawler-manager/backend/crawlers/{site_name}_crawler.py 12345 --workers 10
  
  # ç»„åˆä½¿ç”¨
  python crawler-manager/backend/crawlers/{site_name}_crawler.py 12345 --proxy --workers 10
        """
    )
    
    parser.add_argument(
        'book_id',
        help='ä¹¦ç±IDï¼ˆä»ç½‘ç«™URLä¸­è·å–ï¼‰'
    )
    
    parser.add_argument(
        '-w', '--workers',
        type=int,
        default=5,
        help='å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 5ï¼‰'
    )
    
    parser.add_argument(
        '-p', '--proxy',
        action='store_true',
        help='ä½¿ç”¨ä»£ç†'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
    try:
        crawler = {site_name.capitalize()}Crawler(
            book_id=args.book_id,
            max_workers=args.workers,
            use_proxy=args.proxy
        )
        crawler.run()
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {{e}}")
        sys.exit(1)


if __name__ == '__main__':
    main()
'''
    return template


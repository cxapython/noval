#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ç« çˆ¬è™«XPathè°ƒè¯•å·¥å…·
ç”¨äºå¿«é€Ÿæµ‹è¯•å’Œè°ƒè¯•XPathè¡¨è¾¾å¼
"""
import sys
import json
from pathlib import Path
from scrapy import Selector

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from backend.content_fetcher import ContentFetcher
from backend.config_manager import ConfigManager
from loguru import logger


def debug_xpath(config_file: str, url: str):
    """
    è°ƒè¯•XPathè¡¨è¾¾å¼
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        url: è¦æµ‹è¯•çš„URL
    """
    logger.info("=" * 80)
    logger.info("æ–‡ç« çˆ¬è™«XPathè°ƒè¯•å·¥å…·")
    logger.info("=" * 80)
    
    # åŠ è½½é…ç½®
    config_manager = ConfigManager(config_file)
    
    # è·å–é¡µé¢å†…å®¹
    logger.info(f"ğŸ”— æ­£åœ¨è·å–é¡µé¢: {url}")
    fetcher = ContentFetcher(
        headers=config_manager.get_headers(),
        timeout=config_manager.get_timeout(),
        encoding=config_manager.get_encoding()
    )
    
    html = fetcher.get_page(url)
    if not html:
        logger.error("âŒ è·å–é¡µé¢å¤±è´¥")
        return
    
    logger.success(f"âœ… é¡µé¢è·å–æˆåŠŸï¼Œé•¿åº¦: {len(html)} å­—ç¬¦")
    
    # ä¿å­˜HTMLåˆ°æ–‡ä»¶ä¾›æ£€æŸ¥
    debug_file = Path(__file__).parent.parent / "logs" / "debug_article_page.html"
    debug_file.parent.mkdir(parents=True, exist_ok=True)
    with open(debug_file, 'w', encoding='utf-8') as f:
        f.write(html)
    logger.info(f"ğŸ“ HTMLå·²ä¿å­˜åˆ°: {debug_file}")
    
    # åˆ›å»ºSelector
    selector = Selector(text=html)
    
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“‹ å¼€å§‹æµ‹è¯•XPathè¡¨è¾¾å¼")
    logger.info("=" * 80)
    
    # è·å–é…ç½®
    parsers = config_manager.get_parsers()
    
    # 1. æµ‹è¯• chapter_list
    logger.info("\nã€1ã€‘æµ‹è¯•ç« èŠ‚åˆ—è¡¨ (chapter_list)")
    logger.info("-" * 80)
    
    chapter_list_config = parsers.get('chapter_list', {})
    if not chapter_list_config:
        logger.warning("âš ï¸  æœªé…ç½® chapter_list")
    else:
        # æµ‹è¯• items
        items_config = chapter_list_config.get('items', {})
        if items_config:
            items_xpath = items_config.get('expression', '')
            logger.info(f"Items XPath: {items_xpath}")
            
            try:
                items = selector.xpath(items_xpath).getall()
                logger.success(f"âœ… æ‰¾åˆ° {len(items)} ä¸ªå®¹å™¨")
                
                if items:
                    # æ˜¾ç¤ºå‰3ä¸ª
                    for i, item in enumerate(items[:3], 1):
                        logger.info(f"\nå®¹å™¨ #{i} (å‰200å­—ç¬¦):")
                        logger.info(item[:200] + "..." if len(item) > 200 else item)
                    
                    if len(items) > 3:
                        logger.info(f"\n... è¿˜æœ‰ {len(items) - 3} ä¸ªå®¹å™¨")
                    
                    # åœ¨ç¬¬ä¸€ä¸ªå®¹å™¨ä¸­æµ‹è¯•å­å­—æ®µ
                    logger.info("\n" + "-" * 80)
                    logger.info("ã€åœ¨ç¬¬ä¸€ä¸ªå®¹å™¨ä¸­æµ‹è¯•å­å­—æ®µã€‘")
                    
                    first_item_html = items[0]
                    item_selector = Selector(text=first_item_html)
                    
                    # æµ‹è¯• title
                    title_config = chapter_list_config.get('title', {})
                    if title_config:
                        title_xpath = title_config.get('expression', '')
                        logger.info(f"\nTitle XPath: {title_xpath}")
                        try:
                            titles = item_selector.xpath(title_xpath).getall()
                            if titles:
                                logger.success(f"âœ… æ‰¾åˆ° {len(titles)} ä¸ªæ ‡é¢˜")
                                for i, title in enumerate(titles[:5], 1):
                                    logger.info(f"  {i}. {title}")
                            else:
                                logger.error("âŒ æœªæ‰¾åˆ°æ ‡é¢˜")
                                # å°è¯•å…¶ä»–å¯èƒ½çš„XPath
                                logger.info("\nğŸ’¡ å°è¯•å…¶ä»–å¯èƒ½çš„æ ‡é¢˜é€‰æ‹©å™¨:")
                                suggestions = [
                                    ".//h1/text()",
                                    ".//h2/text()",
                                    ".//h3/text()",
                                    ".//h4/text()",
                                    ".//h5/text()",
                                    ".//a/text()",
                                    ".//div[@class='title']/text()",
                                    ".//span[@class='title']/text()"
                                ]
                                for sugg in suggestions:
                                    result = item_selector.xpath(sugg).getall()
                                    if result:
                                        logger.info(f"  âœ“ {sugg} â†’ {result[:2]}")
                        except Exception as e:
                            logger.error(f"âŒ Title XPath é”™è¯¯: {e}")
                    
                    # æµ‹è¯• url
                    url_config = chapter_list_config.get('url', {})
                    if url_config:
                        url_xpath = url_config.get('expression', '')
                        logger.info(f"\nURL XPath: {url_xpath}")
                        try:
                            urls = item_selector.xpath(url_xpath).getall()
                            if urls:
                                logger.success(f"âœ… æ‰¾åˆ° {len(urls)} ä¸ªURL")
                                for i, u in enumerate(urls[:5], 1):
                                    logger.info(f"  {i}. {u}")
                            else:
                                logger.error("âŒ æœªæ‰¾åˆ°URL")
                                # å°è¯•å…¶ä»–å¯èƒ½çš„XPath
                                logger.info("\nğŸ’¡ å°è¯•å…¶ä»–å¯èƒ½çš„URLé€‰æ‹©å™¨:")
                                suggestions = [
                                    ".//a/@href",
                                    ".//link/@href",
                                    ".//@href",
                                    ".//a[1]/@href"
                                ]
                                for sugg in suggestions:
                                    result = item_selector.xpath(sugg).getall()
                                    if result:
                                        logger.info(f"  âœ“ {sugg} â†’ {result[:2]}")
                        except Exception as e:
                            logger.error(f"âŒ URL XPath é”™è¯¯: {e}")
                
                else:
                    logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•å®¹å™¨")
                    
                    # æä¾›è°ƒè¯•å»ºè®®
                    logger.info("\nğŸ’¡ è°ƒè¯•å»ºè®®:")
                    logger.info("1. æ£€æŸ¥é¡µé¢æ˜¯å¦ä½¿ç”¨JavaScriptåŠ¨æ€åŠ è½½å†…å®¹")
                    logger.info("2. å°è¯•æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼Œå¦‚:")
                    
                    suggestions = [
                        "//ul/li",
                        "//div[contains(@class, 'list')]//div",
                        "//article",
                        "//div[contains(@class, 'item')]",
                        "//div[contains(@class, 'news')]",
                        "//a[contains(@href, 'article')]/..",
                    ]
                    
                    for sugg in suggestions:
                        try:
                            result = selector.xpath(sugg).getall()
                            if result:
                                logger.info(f"  âœ“ {sugg} â†’ æ‰¾åˆ° {len(result)} ä¸ª")
                        except:
                            pass
                    
            except Exception as e:
                logger.error(f"âŒ Items XPath é”™è¯¯: {e}")
    
    # 2. æµ‹è¯•é¡µé¢ç»“æ„åˆ†æ
    logger.info("\n" + "=" * 80)
    logger.info("ã€2ã€‘é¡µé¢ç»“æ„åˆ†æ")
    logger.info("-" * 80)
    
    # ç»Ÿè®¡æ ‡ç­¾
    logger.info("\nå¸¸è§æ ‡ç­¾ç»Ÿè®¡:")
    tags_to_check = ['ul', 'li', 'div', 'article', 'section', 'a', 'h1', 'h2', 'h3', 'h4']
    for tag in tags_to_check:
        count = len(selector.xpath(f'//{tag}').getall())
        if count > 0:
            logger.info(f"  <{tag}>: {count} ä¸ª")
    
    # æ£€æŸ¥å¸¸è§çš„classåç§°
    logger.info("\nå¸¸è§classåç§°:")
    class_patterns = ['list', 'item', 'news', 'article', 'content', 'title', 'link']
    for pattern in class_patterns:
        elements = selector.xpath(f'//*[contains(@class, "{pattern}")]').getall()
        if elements:
            logger.info(f"  å«'{pattern}'çš„å…ƒç´ : {len(elements)} ä¸ª")
    
    # 3. æå–æ‰€æœ‰é“¾æ¥
    logger.info("\nã€3ã€‘é¡µé¢æ‰€æœ‰é“¾æ¥ (å‰20ä¸ª)")
    logger.info("-" * 80)
    all_links = selector.xpath('//a/@href').getall()
    for i, link in enumerate(all_links[:20], 1):
        logger.info(f"  {i}. {link}")
    
    if len(all_links) > 20:
        logger.info(f"\n  ... è¿˜æœ‰ {len(all_links) - 20} ä¸ªé“¾æ¥")
    
    logger.info("\n" + "=" * 80)
    logger.info("è°ƒè¯•å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info(f"\nğŸ’¡ æç¤º:")
    logger.info(f"1. æŸ¥çœ‹ä¿å­˜çš„HTMLæ–‡ä»¶: {debug_file}")
    logger.info(f"2. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¯¥æ–‡ä»¶ï¼Œä½¿ç”¨å¼€å‘è€…å·¥å…·æŸ¥çœ‹ç»“æ„")
    logger.info(f"3. æ ¹æ®ä¸Šé¢çš„å»ºè®®è°ƒæ•´XPathè¡¨è¾¾å¼")
    logger.info(f"4. é‡æ–°è¿è¡Œæ­¤è„šæœ¬éªŒè¯")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ–‡ç« çˆ¬è™«XPathè°ƒè¯•å·¥å…·')
    parser.add_argument('config', help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚: configs/config_tech_163.json')
    parser.add_argument('url', help='è¦æµ‹è¯•çš„URLï¼Œå¦‚: https://tech.163.com/')
    
    args = parser.parse_args()
    
    debug_xpath(args.config, args.url)


if __name__ == '__main__':
    main()


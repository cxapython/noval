#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨å°è¯´çˆ¬è™«æ¡†æ¶ - é…ç½®é©±åŠ¨ç‰ˆ
æ”¯æŒé€šè¿‡JSONé…ç½®æ–‡ä»¶é€‚é…ä¸åŒç½‘ç«™ç»“æ„
"""
import re
import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from urllib.parse import urljoin
from typing import Dict, List, Any, Optional

import requests
from loguru import logger
from scrapy import Selector
from urllib3 import disable_warnings

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from shared.utils.config import DB_CONFIG
from backend.models.database import NovelDatabase
from shared.utils.proxy_utils import ProxyUtils
from redis import Redis

disable_warnings()

REDIS_URL = "redis://@localhost:6379"
redis_cli = Redis.from_url(REDIS_URL)


def safe_int(value, default=0):
    """
    å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°
    :param value: è¦è½¬æ¢çš„å€¼
    :param default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
    :return: æ•´æ•°å€¼
    """
    if isinstance(value, int):
        return value
    if isinstance(value, str):
        try:
            return int(value)
        except (ValueError, TypeError):
            logger.warning(f"âš ï¸  æ— æ³•å°† '{value}' è½¬æ¢ä¸ºæ•´æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default}")
            return default
    if isinstance(value, float):
        return int(value)
    return default


def safe_float(value, default=0.0):
    """
    å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°
    :param value: è¦è½¬æ¢çš„å€¼
    :param default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
    :return: æµ®ç‚¹æ•°å€¼
    """
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        try:
            return float(value)
        except (ValueError, TypeError):
            logger.warning(f"âš ï¸  æ— æ³•å°† '{value}' è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default}")
            return default
    return default


def safe_bool(value, default=False):
    """
    å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºå¸ƒå°”å€¼
    :param value: è¦è½¬æ¢çš„å€¼
    :param default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
    :return: å¸ƒå°”å€¼
    """
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        return value.lower() in ('true', '1', 'yes', 'on')
    if isinstance(value, (int, float)):
        return bool(value)
    return default


class GenericNovelCrawler:
    """é€šç”¨å°è¯´çˆ¬è™« - é…ç½®é©±åŠ¨"""
    
    def __init__(self, config_file: str, book_id: str, max_workers: int = 5, use_proxy: bool = False):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        :param book_id: ä¹¦ç±ID
        :param max_workers: å¹¶å‘çº¿ç¨‹æ•°
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        """
        self.book_id = book_id
        self.config = self._load_config(config_file)
        self.max_workers = max_workers
        self.use_proxy = use_proxy
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–åŸºæœ¬ä¿¡æ¯
        self.site_name = self.config['site_info']['name']
        self.base_url = self.config['site_info']['base_url']
        self.start_url = self._build_url('book_detail', book_id)
        self.headers = self.config.get('request_config', {}).get('headers', {})
        self.timeout = safe_int(self.config.get('request_config', {}).get('timeout', 30), 30)
        
        # æ•°æ®å­˜å‚¨
        self.chapters = []
        self.novel_info = {}
        self.novel_id = None
        self.db = NovelDatabase(**DB_CONFIG)
        
        # ä»£ç†é…ç½®
        self.proxy_utils = None
        if use_proxy:
            self.proxy_utils = ProxyUtils()
            logger.info(f"âœ… å·²å¯ç”¨ä»£ç†")
        
        # å¹¶å‘é…ç½®
        self.progress_lock = Lock()
        self.completed_count = 0
        self.skipped_count = 0
        
        # Redisé…ç½®
        self.redis_cli = redis_cli
        self.redis_success_key = f"novel:success:{self.site_name}:{book_id}"
        self.redis_failed_key = f"novel:failed:{self.site_name}:{book_id}"
        
        logger.info(f"ğŸŒ ç½‘ç«™: {self.site_name}")
        logger.info(f"ğŸ“– ä¹¦ç±ID: {book_id}")
    
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
                return config
        except Exception as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _build_url(self, url_type: str, *args) -> str:
        """æ„å»ºURL"""
        pattern = self.config['url_patterns'].get(url_type, '')
        if not pattern:
            raise ValueError(f"URLæ¨¡å¼ '{url_type}' æœªé…ç½®")
        
        url = pattern.format(*args)
        if not url.startswith('http'):
            url = urljoin(self.base_url, url)
        return url
    
    def _parse_with_config(self, html: str, parser_config: Dict) -> Any:
        """
        æ ¹æ®é…ç½®è§£æHTML
        :param html: HTMLå†…å®¹
        :param parser_config: è§£æå™¨é…ç½®
        :return: è§£æç»“æœ
        """
        # ç±»å‹æ£€æŸ¥
        if not isinstance(parser_config, dict):
            logger.warning(f"âš ï¸  parser_config åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(parser_config).__name__}ï¼Œå€¼ä¸º: {parser_config}")
            return None
        
        parse_type = parser_config.get('type', 'xpath')
        expression = parser_config.get('expression', '')
        index = safe_int(parser_config.get('index', -1), -1)
        default = parser_config.get('default', None)
        post_process = parser_config.get('process', [])
        
        result = None
        
        try:
            if parse_type == 'xpath':
                root = Selector(text=html)
                all_results = root.xpath(expression).getall()
                
                # å¤„ç†ç´¢å¼•ï¼šæ”¯æŒPythonæ ‡å‡†çš„æ­£è´Ÿæ•°ç´¢å¼•
                if index is None or (isinstance(index, int) and index == 999):
                    # None æˆ– 999 è¡¨ç¤ºè·å–æ‰€æœ‰
                    result = all_results
                elif all_results:
                    # ä½¿ç”¨Pythonæ ‡å‡†ç´¢å¼•ï¼š-1=æœ€åä¸€ä¸ª, -2=å€’æ•°ç¬¬äºŒ, 0=ç¬¬ä¸€ä¸ª
                    try:
                        result = all_results[index]
                    except IndexError:
                        logger.warning(f"âš ï¸  ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ï¼Œå…± {len(all_results)} ä¸ªå…ƒç´ ")
                        result = None
                else:
                    result = None
            
            elif parse_type == 'regex':
                matches = re.findall(expression, html)
                
                # å¤„ç†ç´¢å¼•
                if index is None or (isinstance(index, int) and index == 999):
                    # None æˆ– 999 è¡¨ç¤ºè·å–æ‰€æœ‰
                    result = matches
                elif matches:
                    try:
                        result = matches[index]
                    except IndexError:
                        logger.warning(f"âš ï¸  ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ï¼Œå…± {len(matches)} ä¸ªå…ƒç´ ")
                        result = None
                else:
                    result = None
            
            # åº”ç”¨åå¤„ç†
            if result is not None and post_process:
                result = self._apply_post_process(result, post_process)
            
            # å¦‚æœæ²¡æœ‰ç»“æœï¼Œä½¿ç”¨é»˜è®¤å€¼
            if result is None or (isinstance(result, list) and len(result) == 0):
                result = default
                
        except Exception as e:
            logger.warning(f"âš ï¸  è§£æå¤±è´¥: {e}")
            result = default
        
        return result
    
    def _apply_post_process(self, data: Any, processes: List[Dict]) -> Any:
        """
        åº”ç”¨åå¤„ç†
        :param data: åŸå§‹æ•°æ®
        :param processes: å¤„ç†æ­¥éª¤åˆ—è¡¨
        :return: å¤„ç†åçš„æ•°æ®
        """
        result = data
        
        for process in processes:
            method = process.get('method', '')
            params = process.get('params', {})
            
            try:
                if method == 'strip':
                    if isinstance(result, str):
                        result = result.strip(params.get('chars', None))
                    elif isinstance(result, list):
                        result = [item.strip(params.get('chars', None)) if isinstance(item, str) else item for item in result]
                
                elif method == 'replace':
                    old = params.get('old', '')
                    new = params.get('new', '')
                    if isinstance(result, str):
                        result = result.replace(old, new)
                    elif isinstance(result, list):
                        result = [item.replace(old, new) if isinstance(item, str) else item for item in result]
                
                elif method == 'regex_replace':
                    pattern = params.get('pattern', '')
                    repl = params.get('repl', '')
                    if isinstance(result, str):
                        result = re.sub(pattern, repl, result)
                    elif isinstance(result, list):
                        result = [re.sub(pattern, repl, item) if isinstance(item, str) else item for item in result]
                
                elif method == 'join':
                    if isinstance(result, list):
                        separator = params.get('separator', '')
                        result = separator.join([str(item) for item in result])
                
                elif method == 'split':
                    if isinstance(result, str):
                        separator = params.get('separator', ' ')
                        result = result.split(separator)
                
                elif method == 'extract_first':
                    if isinstance(result, list) and len(result) > 0:
                        result = result[0]
                
                elif method == 'extract_index':
                    if isinstance(result, list):
                        idx = params.get('index', 0)
                        if len(result) > idx:
                            result = result[idx]
            
            except Exception as e:
                logger.warning(f"âš ï¸  åå¤„ç†å¤±è´¥ ({method}): {e}")
        
        return result
    
    def get_page(self, url: str, max_retries: int = 20) -> Optional[str]:
        """
        è·å–ç½‘é¡µå†…å®¹ï¼ˆå¸¦é‡è¯•ï¼‰
        :param url: ç›®æ ‡URL
        :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        :return: HTMLå†…å®¹
        """
        proxies = None
        
        for i in range(max_retries):
            try:
                if self.use_proxy and self.proxy_utils:
                    proxies = self.proxy_utils.get_proxy()
                
                response = requests.get(
                    url,
                    headers=self.headers,
                    proxies=proxies,
                    timeout=self.timeout,
                    verify=False
                )
                
                # å¤„ç†ç¼–ç 
                encoding = self.config.get('request_config', {}).get('encoding', None)
                if encoding:
                    response.encoding = encoding
                else:
                    response.encoding = response.apparent_encoding or 'utf-8'
                
                if response.status_code == 200:
                    return response.text
                    
            except Exception as e:
                if i == max_retries - 1:
                    logger.warning(f"âš ï¸  è·å–é¡µé¢å¤±è´¥ ({max_retries}æ¬¡): {url[:50]}...")
        
        return None
    
    def is_chapter_downloaded(self, chapter_url: str) -> bool:
        """æ£€æŸ¥ç« èŠ‚æ˜¯å¦å·²ä¸‹è½½"""
        try:
            return self.redis_cli.sismember(self.redis_success_key, chapter_url)
        except Exception as e:
            logger.warning(f"âš ï¸  Redisæ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def mark_chapter_success(self, chapter_url: str):
        """æ ‡è®°ç« èŠ‚ä¸‹è½½æˆåŠŸ"""
        try:
            self.redis_cli.sadd(self.redis_success_key, chapter_url)
            self.redis_cli.srem(self.redis_failed_key, chapter_url)
            self.redis_cli.expire(self.redis_success_key, 30 * 24 * 3600)
        except Exception as e:
            logger.warning(f"âš ï¸  Redisè®°å½•æˆåŠŸå¤±è´¥: {e}")
    
    def mark_chapter_failed(self, chapter_url: str):
        """æ ‡è®°ç« èŠ‚ä¸‹è½½å¤±è´¥"""
        try:
            self.redis_cli.sadd(self.redis_failed_key, chapter_url)
            self.redis_cli.expire(self.redis_failed_key, 7 * 24 * 3600)
        except Exception as e:
            logger.warning(f"âš ï¸  Redisè®°å½•å¤±è´¥å¤±è´¥: {e}")
    
    def get_download_stats(self):
        """è·å–ä¸‹è½½ç»Ÿè®¡"""
        try:
            success_count = self.redis_cli.scard(self.redis_success_key)
            failed_count = self.redis_cli.scard(self.redis_failed_key)
            return success_count, failed_count
        except Exception as e:
            logger.warning(f"âš ï¸  Redisè·å–ç»Ÿè®¡å¤±è´¥: {e}")
            return 0, 0
    
    def clear_failed_records(self):
        """æ¸…é™¤å¤±è´¥è®°å½•"""
        try:
            count = self.redis_cli.scard(self.redis_failed_key)
            if count > 0:
                self.redis_cli.delete(self.redis_failed_key)
                logger.info(f"ğŸ—‘ï¸  å·²æ¸…é™¤ {count} æ¡å¤±è´¥è®°å½•")
        except Exception as e:
            logger.warning(f"âš ï¸  æ¸…é™¤å¤±è´¥è®°å½•å¤±è´¥: {e}")
    
    def parse_novel_info(self, html: str) -> Dict:
        """è§£æå°è¯´ä¿¡æ¯"""
        novel_info = {}
        parsers = self.config.get('parsers', {}).get('novel_info', {})
        
        # éªŒè¯é…ç½®ç±»å‹
        if not isinstance(parsers, dict):
            logger.error(f"âŒ novel_info é…ç½®åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(parsers).__name__}")
            return novel_info
        
        for field, parser_config in parsers.items():
            # è·³è¿‡æ³¨é‡Šå­—æ®µ
            if field.startswith('_'):
                continue
            
            try:
                value = self._parse_with_config(html, parser_config)
                novel_info[field] = value
            except Exception as e:
                logger.warning(f"âš ï¸  è§£æå­—æ®µ {field} å¤±è´¥: {e}")
                novel_info[field] = None
        
        return novel_info
    
    def parse_chapter_list(self) -> bool:
        """è§£æç« èŠ‚åˆ—è¡¨"""
        logger.info(f"ğŸ“– å¼€å§‹è·å–ç« èŠ‚åˆ—è¡¨...")
        logger.info(f"ğŸ”— å°è¯´åœ°å€: {self.start_url}")
        
        # è·å–é¦–é¡µ
        html = self.get_page(self.start_url)
        if not html:
            logger.error("âŒ è·å–é¦–é¡µå¤±è´¥")
            return False
        
        # è§£æå°è¯´ä¿¡æ¯
        self.novel_info = self.parse_novel_info(html)
        
        if not self.novel_info.get('title'):
            logger.error("âŒ è§£æå°è¯´ä¿¡æ¯å¤±è´¥")
            return False
        
        logger.info(f"ğŸ“š å°è¯´åç§°: {self.novel_info.get('title')}")
        logger.info(f"âœï¸  ä½œè€…: {self.novel_info.get('author', 'æœªçŸ¥')}")
        
        # è§£æç« èŠ‚åˆ—è¡¨é…ç½®
        chapter_list_config = self.config['parsers']['chapter_list']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é¡µ
        pagination_config = chapter_list_config.get('pagination')
        if pagination_config and safe_bool(pagination_config.get('enabled', False), False):
            # æœ‰åˆ†é¡µ
            max_page = self._get_max_page(html, pagination_config)
            logger.info(f"ğŸ“„ å…± {max_page} é¡µç« èŠ‚åˆ—è¡¨")
            
            for page in range(1, max_page + 1):
                if page == 1:
                    page_html = html
                else:
                    page_url = self._build_pagination_url(page, pagination_config)
                    logger.info(f"ğŸ“„ è·å–ç¬¬ {page} é¡µ: {page_url}")
                    page_html = self.get_page(page_url)
                    
                    if not page_html:
                        logger.warning(f"âš ï¸  ç¬¬ {page} é¡µè·å–å¤±è´¥")
                        break
                
                chapters = self._parse_chapters_from_page(page_html, chapter_list_config)
                self.chapters.extend(chapters)
                logger.info(f"   âœ“ æœ¬é¡µè·å– {len(chapters)} ç« ï¼Œç´¯è®¡ {len(self.chapters)} ç« ")
        else:
            # æ— åˆ†é¡µ
            chapters = self._parse_chapters_from_page(html, chapter_list_config)
            self.chapters.extend(chapters)
        
        logger.info(f"\nâœ… ç« èŠ‚åˆ—è¡¨è·å–å®Œæˆï¼Œå…± {len(self.chapters)} ç« \n")
        return True
    
    def _get_max_page(self, html: str, pagination_config: Dict) -> int:
        """è·å–æœ€å¤§é¡µæ•°"""
        max_page_config = pagination_config.get('max_page')
        if max_page_config:
            result = self._parse_with_config(html, max_page_config)
            if result:
                # å¯èƒ½éœ€è¦ä»æ–‡æœ¬ä¸­æå–æ•°å­—
                if isinstance(result, str):
                    numbers = re.findall(r'\d+', result)
                    if numbers:
                        return int(numbers[0])
                elif isinstance(result, (int, float)):
                    return int(result)
        return 1
    
    def _build_pagination_url(self, page: int, pagination_config: Dict) -> str:
        """æ„å»ºåˆ†é¡µURL"""
        url_pattern = pagination_config.get('url_pattern', '')
        return url_pattern.format(base_url=self.base_url, book_id=self.book_id, page=page)
    
    def _parse_chapters_from_page(self, html: str, chapter_list_config: Dict) -> List[Dict]:
        """ä»é¡µé¢è§£æç« èŠ‚åˆ—è¡¨"""
        chapters = []
        
        # éªŒè¯é…ç½®ç±»å‹
        if not isinstance(chapter_list_config, dict):
            raise TypeError(f"chapter_list_config åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(chapter_list_config).__name__}")
        
        # è·å–ç« èŠ‚é¡¹é…ç½®
        items_config = chapter_list_config.get('items')
        title_config = chapter_list_config.get('title')
        url_config = chapter_list_config.get('url')
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if not items_config:
            raise ValueError("chapter_list_config ç¼ºå°‘ 'items' å­—æ®µ")
        if not title_config:
            raise ValueError("chapter_list_config ç¼ºå°‘ 'title' å­—æ®µ")
        if not url_config:
            raise ValueError("chapter_list_config ç¼ºå°‘ 'url' å­—æ®µ")
        
        # éªŒè¯å­—æ®µç±»å‹
        if not isinstance(items_config, dict):
            raise TypeError(f"items é…ç½®åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(items_config).__name__}")
        if not isinstance(title_config, dict):
            raise TypeError(f"title é…ç½®åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(title_config).__name__}")
        if not isinstance(url_config, dict):
            raise TypeError(f"url é…ç½®åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(url_config).__name__}")
        
        # å…ˆè·å–æ‰€æœ‰ç« èŠ‚é¡¹
        root = Selector(text=html)
        items_xpath = items_config.get('expression', '')
        if not items_xpath:
            raise ValueError("items é…ç½®ç¼ºå°‘ 'expression' å­—æ®µ")
        
        chapter_items = root.xpath(items_xpath)
        
        for item in chapter_items:
            try:
                # è§£ææ ‡é¢˜
                title_expr = title_config.get('expression', '')
                if not title_expr:
                    continue
                title = item.xpath(title_expr).get()
                
                # è§£æURL
                url_expr = url_config.get('expression', '')
                if not url_expr:
                    continue
                url = item.xpath(url_expr).get()
                
                if title and url:
                    # åå¤„ç†
                    if title_config.get('process'):
                        title = self._apply_post_process(title, title_config['process'])
                    
                    # æ„å»ºå®Œæ•´URL
                    chapter_url = urljoin(self.base_url, url)
                    
                    chapters.append({
                        'title': title,
                        'url': chapter_url,
                        'content': ''
                    })
            except Exception as e:
                logger.warning(f"âš ï¸  è§£æç« èŠ‚é¡¹å¤±è´¥: {e}")
                continue
        
        return chapters
    
    def download_chapter_content(self, chapter_url: str) -> str:
        """
        ä¸‹è½½ç« èŠ‚å†…å®¹ï¼ˆæ”¯æŒå¤šé¡µï¼‰
        :param chapter_url: ç« èŠ‚URL
        :return: å®Œæ•´å†…å®¹
        """
        all_content = []
        current_url = chapter_url
        page_num = 1
        max_pages = safe_int(self.config['parsers']['chapter_content'].get('max_pages', 50), 50)  # é˜²æ­¢æ— é™å¾ªç¯
        
        content_config = self.config['parsers']['chapter_content']['content']
        next_page_config = self.config['parsers']['chapter_content'].get('next_page')
        clean_config = self.config['parsers']['chapter_content'].get('clean', [])
        
        while current_url and page_num <= max_pages:
            html = self.get_page(current_url)
            if not html:
                logger.warning(f"âš ï¸  ç¬¬{page_num}é¡µè·å–å¤±è´¥")
                break
            
            # è§£æå†…å®¹
            content = self._parse_with_config(html, content_config)
            if content:
                if isinstance(content, list):
                    content = '\n'.join([str(c).strip() for c in content if str(c).strip()])
                all_content.append(content)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            if next_page_config and safe_bool(next_page_config.get('enabled', False), False):
                next_url = self._parse_with_config(html, next_page_config)
                if next_url and next_url != current_url:
                    current_url = urljoin(self.base_url, next_url)
                    page_num += 1
                else:
                    break
            else:
                break
        
        # åˆå¹¶å†…å®¹
        final_content = '\n\n'.join(all_content) if all_content else ''
        
        # æ¸…ç†å†…å®¹
        if clean_config:
            for clean_rule in clean_config:
                final_content = self._apply_post_process(final_content, [clean_rule])
        
        return final_content
    
    def download_and_save_chapter(self, index: int) -> bool:
        """ä¸‹è½½å¹¶ä¿å­˜å•ä¸ªç« èŠ‚"""
        chapter = self.chapters[index]
        chapter_url = chapter['url']
        chapter_title = chapter['title']
        
        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        if self.is_chapter_downloaded(chapter_url):
            with self.progress_lock:
                self.skipped_count += 1
                self.completed_count += 1
                progress = (self.completed_count / len(self.chapters)) * 100
                logger.info(
                    f"â­ï¸  [{self.completed_count}/{len(self.chapters)}] {chapter_title} (å·²ä¸‹è½½,è·³è¿‡) - è¿›åº¦: {progress:.1f}%"
                )
            return True
        
        # ä¸‹è½½å†…å®¹
        content = self.download_chapter_content(chapter_url)
        chapter['content'] = content
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
        if not content or len(content.strip()) == 0:
            logger.error(f"âŒ {chapter_title} å†…å®¹ä¸ºç©º")
            self.mark_chapter_failed(chapter_url)
            with self.progress_lock:
                self.completed_count += 1
            return False
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        download_success = False
        db = NovelDatabase(**DB_CONFIG, use_pool=True, silent=True)
        if db.connect():
            try:
                db.insert_chapter(
                    self.novel_id,
                    index + 1,
                    chapter_title,
                    content,
                    chapter_url
                )
                db.close()
                download_success = True
            except Exception as e:
                logger.error(f"âŒ {chapter_title} æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
                db.close()
                download_success = False
        
        # æ›´æ–°Redisè®°å½•
        if download_success:
            self.mark_chapter_success(chapter_url)
            status_icon = "âœ…"
        else:
            self.mark_chapter_failed(chapter_url)
            status_icon = "âŒ"
        
        # æ›´æ–°è¿›åº¦
        with self.progress_lock:
            self.completed_count += 1
            progress = (self.completed_count / len(self.chapters)) * 100
            logger.info(
                f"{status_icon} [{self.completed_count}/{len(self.chapters)}] {chapter_title} ({len(content)} å­—) - è¿›åº¦: {progress:.1f}%"
            )
        
        # å»¶è¿Ÿ
        delay = safe_float(self.config.get('crawler_config', {}).get('delay', 0.3), 0.3)
        time.sleep(delay)
        
        return download_success
    
    def download_all_chapters(self, retry_failed: bool = False) -> bool:
        """
        å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½æ‰€æœ‰ç« èŠ‚
        :param retry_failed: æ˜¯å¦é‡è¯•å¤±è´¥çš„ç« èŠ‚
        :return: æ˜¯å¦æˆåŠŸ
        """
        if not self.parse_chapter_list():
            logger.error("âŒ è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥")
            return False
        
        # æ˜¾ç¤ºç»Ÿè®¡
        success_count, failed_count = self.get_download_stats()
        logger.info(f"ğŸ“Š Redisç»Ÿè®¡: å·²æˆåŠŸ {success_count} ç« ï¼Œå¤±è´¥ {failed_count} ç« ")
        
        if retry_failed and failed_count > 0:
            logger.info(f"ğŸ”„ å‡†å¤‡é‡è¯• {failed_count} ä¸ªå¤±è´¥çš„ç« èŠ‚")
            self.clear_failed_records()
        
        # è¿æ¥æ•°æ®åº“
        if not self.db.connect():
            logger.error("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
            return False
        
        # æ£€æŸ¥å°è¯´æ˜¯å¦å·²å­˜åœ¨
        existing_novel = self.db.get_novel_by_url(self.start_url)
        if existing_novel:
            self.novel_id = existing_novel['id']
            logger.info(f"ğŸ“š å°è¯´å·²å­˜åœ¨ (ID: {self.novel_id})ï¼Œå°†æ›´æ–°ç« èŠ‚\n")
        else:
            # æ’å…¥å°è¯´ä¿¡æ¯
            self.novel_id = self.db.insert_novel(
                self.novel_info.get('title'),
                self.novel_info.get('author', 'æœªçŸ¥'),
                self.start_url,
                cover_url=self.novel_info.get('cover', '')
            )
            if not self.novel_id:
                logger.error("âŒ ä¿å­˜å°è¯´ä¿¡æ¯å¤±è´¥")
                return False
            logger.info(f"âœ… å°è¯´ä¿¡æ¯å·²ä¿å­˜ (ID: {self.novel_id})\n")
        
        self.db.close()
        
        # å¤šçº¿ç¨‹ä¸‹è½½
        logger.info("=" * 60)
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘ä¸‹è½½ç« èŠ‚å†…å®¹ (çº¿ç¨‹æ•°: {self.max_workers})")
        logger.info("=" * 60)
        
        self.completed_count = 0
        self.skipped_count = 0
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.download_and_save_chapter, i): i
                      for i in range(len(self.chapters))}
            
            for future in as_completed(futures):
                index = futures[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"âŒ ç« èŠ‚ {index + 1} ä¸‹è½½å¤±è´¥: {e}")
        
        elapsed_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡
        if self.db.connect():
            self.db.update_novel_stats(self.novel_id)
            self.db.close()
        
        final_success, final_failed = self.get_download_stats()
        new_downloads = self.completed_count - self.skipped_count
        
        logger.info("\n" + "=" * 60)
        logger.info(f"âœ… æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆï¼")
        logger.info(f"   æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"   æ€»ç« èŠ‚: {len(self.chapters)}")
        logger.info(f"   æ–°ä¸‹è½½: {new_downloads} ç« ")
        logger.info(f"   è·³è¿‡(å·²ä¸‹è½½): {self.skipped_count} ç« ")
        logger.info(f"   æˆåŠŸ: {final_success} ç« ")
        logger.info(f"   å¤±è´¥: {final_failed} ç« ")
        logger.info("=" * 60)
        
        return True
    
    def retry_failed_chapters(self) -> bool:
        """åªé‡è¯•å¤±è´¥çš„ç« èŠ‚"""
        logger.info("=" * 60)
        logger.info("ğŸ”„ å¼€å§‹é‡è¯•å¤±è´¥çš„ç« èŠ‚")
        logger.info("=" * 60)
        
        try:
            # è·å–å¤±è´¥ç« èŠ‚URL
            failed_urls = self.redis_cli.smembers(self.redis_failed_key)
            if not failed_urls:
                logger.info("âœ… æ²¡æœ‰å¤±è´¥çš„ç« èŠ‚éœ€è¦é‡è¯•")
                return True
            
            failed_urls = [url.decode('utf-8') if isinstance(url, bytes) else url for url in failed_urls]
            logger.info(f"ğŸ“‹ å…±æœ‰ {len(failed_urls)} ä¸ªå¤±è´¥ç« èŠ‚éœ€è¦é‡è¯•")
            
            # è§£æç« èŠ‚åˆ—è¡¨
            if not self.parse_chapter_list():
                logger.error("âŒ è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥")
                return False
            
            # è·å–å°è¯´ID
            if not self.db.connect():
                logger.error("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
                return False
            
            existing_novel = self.db.get_novel_by_url(self.start_url)
            if existing_novel:
                self.novel_id = existing_novel['id']
            else:
                logger.error("âŒ å°è¯´ä¸å­˜åœ¨")
                self.db.close()
                return False
            
            self.db.close()
            
            # ç­›é€‰éœ€è¦é‡è¯•çš„ç« èŠ‚
            retry_chapters = []
            for idx, chapter in enumerate(self.chapters):
                if chapter['url'] in failed_urls:
                    retry_chapters.append((idx, chapter))
            
            logger.info(f"ğŸ¯ åŒ¹é…åˆ° {len(retry_chapters)} ä¸ªç« èŠ‚éœ€è¦é‡è¯•")
            
            if not retry_chapters:
                logger.info("âœ… æ²¡æœ‰åŒ¹é…çš„ç« èŠ‚éœ€è¦é‡è¯•")
                return True
            
            # æ¸…é™¤å¤±è´¥è®°å½•
            self.clear_failed_records()
            
            # å¤šçº¿ç¨‹é‡è¯•
            self.completed_count = 0
            self.skipped_count = 0
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {executor.submit(self.download_and_save_chapter, idx): idx
                          for idx, chapter in retry_chapters}
                
                for future in as_completed(futures):
                    index = futures[future]
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"âŒ ç« èŠ‚ {index + 1} é‡è¯•å¤±è´¥: {e}")
            
            elapsed_time = time.time() - start_time
            final_success, final_failed = self.get_download_stats()
            
            logger.info("\n" + "=" * 60)
            logger.info(f"âœ… é‡è¯•å®Œæˆï¼")
            logger.info(f"   è€—æ—¶: {elapsed_time:.2f} ç§’")
            logger.info(f"   é‡è¯•ç« èŠ‚: {len(retry_chapters)}")
            logger.info(f"   å½“å‰æˆåŠŸ: {final_success} ç« ")
            logger.info(f"   å½“å‰å¤±è´¥: {final_failed} ç« ")
            logger.info("=" * 60)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‡è¯•å¤±è´¥ç« èŠ‚æ—¶å‡ºé”™: {e}")
            return False
    
    def print_summary(self):
        """æ‰“å°ä¸‹è½½æ‘˜è¦"""
        success_count, failed_count = self.get_download_stats()
        
        logger.info(f"\n{'=' * 60}")
        logger.info(f"âœ… ä¸‹è½½å®Œæˆï¼")
        logger.info(f"{'=' * 60}")
        logger.info(f"å°è¯´ID: {self.novel_id}")
        logger.info(f"å°è¯´åç§°: {self.novel_info.get('title')}")
        logger.info(f"ä½œè€…: {self.novel_info.get('author', 'æœªçŸ¥')}")
        logger.info(f"ç« èŠ‚æ€»æ•°: {len(self.chapters)}")
        logger.info(f"æˆåŠŸç« èŠ‚: {success_count}")
        logger.info(f"å¤±è´¥ç« èŠ‚: {failed_count}")
        
        total_words = sum(len(ch['content']) for ch in self.chapters)
        logger.info(f"æ€»å­—æ•°: {total_words:,} å­—")
        logger.info(f"{'=' * 60}")


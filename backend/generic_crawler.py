#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨å°è¯´çˆ¬è™«æ¡†æ¶ - æ¨¡å—åŒ–ç‰ˆæœ¬
èŒè´£æ‹†åˆ†ï¼š
- ConfigManager: é…ç½®ç®¡ç†
- HtmlParser: HTMLè§£æ
- ContentFetcher: HTTPè¯·æ±‚
- GenericNovelCrawler: æ ¸å¿ƒçˆ¬è™«é€»è¾‘å’Œä»»åŠ¡åè°ƒ
"""
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from threading import Lock
from typing import Dict, List
from urllib.parse import urljoin, urlparse

from loguru import logger
from redis import Redis
from scrapy import Selector

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from shared.utils.config import DB_CONFIG
from backend.models.database import NovelDatabase
from shared.utils.proxy_utils import ProxyUtils
from backend.config_manager import ConfigManager
from backend.parser import HtmlParser
from backend.content_fetcher import ContentFetcher

REDIS_URL = "redis://@localhost:6379"
redis_cli = Redis.from_url(REDIS_URL)


class GenericNovelCrawler:
    """é€šç”¨å°è¯´çˆ¬è™« - æ¨¡å—åŒ–ç‰ˆæœ¬"""

    def __init__(self, config_file: str, book_id: str, max_workers: int = 5, use_proxy: bool = False,
                 progress_callback=None, log_callback=None, stop_flag=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        :param book_id: ä¹¦ç±ID
        :param max_workers: å¹¶å‘çº¿ç¨‹æ•°
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        :param progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (total, completed, failed, current_chapter)
        :param log_callback: æ—¥å¿—å›è°ƒå‡½æ•° (level, message)
        :param stop_flag: åœæ­¢æ ‡å¿— (threading.Event)
        """
        self.book_id = book_id
        self.max_workers = max_workers
        self.use_proxy = use_proxy

        # å›è°ƒå‡½æ•°
        self.progress_callback = progress_callback
        self.log_callback = log_callback
        self.stop_flag = stop_flag

        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager(config_file)
        site_info = self.config_manager.get_site_info()

        self.site_name = site_info.get('name')
        self.base_url = site_info.get('base_url')
        self.start_url = self.config_manager.build_url('book_detail', book_id=book_id)
        self.url_templates = self.config_manager.get_url_templates()

        # åˆå§‹åŒ–HTMLè§£æå™¨
        self.parser = HtmlParser(self.base_url)

        # åˆå§‹åŒ–ä»£ç†å·¥å…·
        proxy_utils = None
        if use_proxy:
            proxy_utils = ProxyUtils()
            self._log('INFO', "âœ… å·²å¯ç”¨ä»£ç†")

        # åˆå§‹åŒ–å†…å®¹è·å–å™¨
        self.fetcher = ContentFetcher(
            headers=self.config_manager.get_headers(),
            timeout=self.config_manager.get_timeout(),
            encoding=self.config_manager.get_encoding(),
            proxy_utils=proxy_utils
        )

        # æ•°æ®å­˜å‚¨
        self.chapters = []
        self.novel_info = {}
        self.novel_id = None
        self.db = NovelDatabase(**DB_CONFIG)

        # å¹¶å‘é…ç½®
        self.progress_lock = Lock()
        self.completed_count = 0
        self.skipped_count = 0

        # Redisé…ç½®
        self.redis_cli = redis_cli
        self.redis_success_key = f"novel:success:{self.site_name}:{book_id}"
        self.redis_failed_key = f"novel:failed:{self.site_name}:{book_id}"

        self._log('INFO', f"ğŸŒ ç½‘ç«™: {self.site_name}")
        self._log('INFO', f"ğŸ“– ä¹¦ç±ID: {book_id}")

    def _log(self, level: str, message: str):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
        logger_func = {
            'INFO': logger.info,
            'WARNING': logger.warning,
            'ERROR': logger.error,
            'SUCCESS': logger.success
        }.get(level, logger.info)

        logger_func(message)

        # è°ƒç”¨æ—¥å¿—å›è°ƒ
        if self.log_callback:
            try:
                self.log_callback(level, message)
            except Exception as e:
                logger.error(f"æ—¥å¿—å›è°ƒå¤±è´¥: {e}")

    def _update_progress(self, **kwargs):
        """æ›´æ–°è¿›åº¦"""
        if self.progress_callback:
            try:
                self.progress_callback(**kwargs)
            except Exception as e:
                logger.error(f"è¿›åº¦å›è°ƒå¤±è´¥: {e}")

    def _check_stop(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢"""
        if self.stop_flag and self.stop_flag.is_set():
            return True
        return False

    def is_chapter_downloaded(self, chapter_url: str) -> bool:
        """æ£€æŸ¥ç« èŠ‚æ˜¯å¦å·²ä¸‹è½½"""
        try:
            return self.redis_cli.sismember(self.redis_success_key, chapter_url)
        except Exception as e:
            logger.warning(f"âš ï¸  Redisæ£€æŸ¥å¤±è´¥: {e}")
            return False

    def mark_chapter_success(self, chapter_url: str):
        """æ ‡è®°ç« èŠ‚ä¸‹è½½æˆåŠŸ"""
        try:
            self.redis_cli.sadd(self.redis_success_key, chapter_url)
            self.redis_cli.srem(self.redis_failed_key, chapter_url)
            self.redis_cli.expire(self.redis_success_key, 30 * 24 * 3600)
        except Exception as e:
            logger.warning(f"âš ï¸  Redisè®°å½•æˆåŠŸå¤±è´¥: {e}")

    def mark_chapter_failed(self, chapter_url: str):
        """æ ‡è®°ç« èŠ‚ä¸‹è½½å¤±è´¥"""
        try:
            self.redis_cli.sadd(self.redis_failed_key, chapter_url)
            self.redis_cli.expire(self.redis_failed_key, 7 * 24 * 3600)
        except Exception as e:
            logger.warning(f"âš ï¸  Redisè®°å½•å¤±è´¥å¤±è´¥: {e}")

    def get_download_stats(self):
        """è·å–ä¸‹è½½ç»Ÿè®¡"""
        try:
            success_count = self.redis_cli.scard(self.redis_success_key)
            failed_count = self.redis_cli.scard(self.redis_failed_key)
            return success_count, failed_count
        except Exception as e:
            logger.warning(f"âš ï¸  Redisè·å–ç»Ÿè®¡å¤±è´¥: {e}")
            return 0, 0

    def clear_failed_records(self):
        """æ¸…é™¤å¤±è´¥è®°å½•"""
        try:
            count = self.redis_cli.scard(self.redis_failed_key)
            if count > 0:
                self.redis_cli.delete(self.redis_failed_key)
                logger.info(f"ğŸ—‘ï¸  å·²æ¸…é™¤ {count} æ¡å¤±è´¥è®°å½•")
        except Exception as e:
            logger.warning(f"âš ï¸  æ¸…é™¤å¤±è´¥è®°å½•å¤±è´¥: {e}")

    def parse_novel_info(self, html: str) -> Dict:
        """è§£æå°è¯´ä¿¡æ¯"""
        novel_info = {}
        parsers = self.config_manager.get_parsers().get('novel_info', {})

        # éªŒè¯é…ç½®ç±»å‹
        if not isinstance(parsers, dict):
            logger.error(f"âŒ novel_info é…ç½®åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(parsers).__name__}")
            return novel_info

        for field, parser_config in parsers.items():
            # è·³è¿‡æ³¨é‡Šå­—æ®µ
            if field.startswith('_'):
                continue

            try:
                value = self.parser.parse_with_config(html, parser_config)
                novel_info[field] = value
            except Exception as e:
                logger.warning(f"âš ï¸  è§£æå­—æ®µ {field} å¤±è´¥: {e}")
                novel_info[field] = None

        return novel_info

    def parse_chapter_list(self) -> bool:
        """è§£æç« èŠ‚åˆ—è¡¨"""
        self._log('INFO', f"ğŸ“– å¼€å§‹è·å–ç« èŠ‚åˆ—è¡¨...")
        self._log('INFO', f"ğŸ”— å°è¯´åœ°å€: {self.start_url}")

        # è·å–é¦–é¡µ
        html = self.fetcher.get_page(self.start_url,
                                     max_retries=self.config_manager.get_max_retries())
        if not html:
            self._log('ERROR', "âŒ è·å–é¦–é¡µå¤±è´¥")
            return False

        # è§£æå°è¯´ä¿¡æ¯
        self.novel_info = self.parse_novel_info(html)

        if not self.novel_info.get('title'):
            self._log('ERROR', "âŒ è§£æå°è¯´ä¿¡æ¯å¤±è´¥")
            return False

        self._log('INFO', f"ğŸ“š å°è¯´åç§°: {self.novel_info.get('title')}")
        self._log('INFO', f"âœï¸  ä½œè€…: {self.novel_info.get('author', 'æœªçŸ¥')}")

        # è§£æç« èŠ‚åˆ—è¡¨é…ç½®
        parsers = self.config_manager.get_parsers()
        chapter_list_config = parsers.get('chapter_list', {})

        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é¡µ
        pagination_config = chapter_list_config.get('pagination')
        if pagination_config and pagination_config.get('enabled', False):
            # æœ‰åˆ†é¡µ - ä½¿ç”¨ url_templates.chapter_list_page æ„å»ºç¿»é¡µURL
            max_page = self._get_max_page(html, pagination_config)
            logger.info(f"ğŸ“„ å…± {max_page} é¡µç« èŠ‚åˆ—è¡¨")

            for page in range(1, max_page + 1):
                if page == 1:
                    page_html = html
                else:
                    # ä½¿ç”¨ url_templates.chapter_list_page æ„å»ºURL
                    page_url = self._build_pagination_url(page)
                    logger.info(f"ğŸ“„ è·å–ç¬¬ {page} é¡µ: {page_url}")
                    page_html = self.fetcher.get_page(page_url,
                                                      max_retries=self.config_manager.get_max_retries())

                    if not page_html:
                        logger.warning(f"âš ï¸  ç¬¬ {page} é¡µè·å–å¤±è´¥")
                        break

                chapters = self._parse_chapters_from_page(page_html, chapter_list_config)
                self.chapters.extend(chapters)
                logger.info(f"   âœ“ æœ¬é¡µè·å– {len(chapters)} ç« ï¼Œç´¯è®¡ {len(self.chapters)} ç« ")
        else:
            # æ— åˆ†é¡µ
            chapters = self._parse_chapters_from_page(html, chapter_list_config)
            self.chapters.extend(chapters)

        logger.info(f"\nâœ… ç« èŠ‚åˆ—è¡¨è·å–å®Œæˆï¼Œå…± {len(self.chapters)} ç« \n")
        return True

    def _get_max_page(self, html: str, pagination_config: Dict) -> int:
        """
        è·å–ç« èŠ‚åˆ—è¡¨çš„æœ€å¤§é¡µæ•°
        :param html: HTMLå†…å®¹
        :param pagination_config: åˆ†é¡µé…ç½®
        :return: æœ€å¤§é¡µæ•°
        """
        # è·å–æ‰‹åŠ¨é…ç½®çš„æœ€å¤§é¡µæ•°ï¼Œå…¼å®¹æ—§é…ç½®
        max_page_manual = pagination_config.get('max_page_manual', 100)

        # è·å–xpathé…ç½®ï¼Œå…¼å®¹æ—§çš„max_pageå­—æ®µ
        max_page_xpath_config = pagination_config.get('max_page_xpath') or pagination_config.get('max_page')

        # å¤ç”¨ç« èŠ‚å†…å®¹çš„æå–é€»è¾‘
        return self._extract_max_pages_from_html(html, max_page_xpath_config, max_page_manual)

    def _build_pagination_url(self, page: int = 2) -> str:
        """
        æ„å»ºç« èŠ‚åˆ—è¡¨åˆ†é¡µURLï¼ˆä»ç¬¬2é¡µå¼€å§‹ï¼‰
        ä½¿ç”¨ url_templates.chapter_list_page é…ç½®
        """
        return self.config_manager.build_url('chapter_list_page', book_id=self.book_id, page=page)

    def _build_content_next_page_url(self, chapter_url: str, page: int, next_page_config: Dict = None) -> str:
        """
        æ„å»ºç« èŠ‚å†…å®¹ç¿»é¡µURLï¼ˆä»ç¬¬2é¡µå¼€å§‹ï¼‰
        ä½¿ç”¨ url_templates.chapter_content_page é…ç½®
        
        :param chapter_url: ç« èŠ‚URLï¼Œç”¨äºæå–book_idå’Œchapter_id
        :param page: é¡µç ï¼ˆâ‰¥2ï¼‰
        :param next_page_config: next_pageé…ç½®ï¼ˆå¯é€‰ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
        :return: ä¸‹ä¸€é¡µURLï¼Œå¦‚æœæ— æ³•æ„å»ºåˆ™è¿”å›None
        """
        # ä»chapter_urlä¸­æå–book_idå’Œchapter_id
        # å…ˆå»é™¤åè®®å’ŒåŸŸåï¼Œåªæå–è·¯å¾„ä¸­çš„æ•°å­—ï¼ˆé¿å…æå–åŸŸåä¸­çš„æ•°å­—å¦‚djks5.comä¸­çš„5ï¼‰
        parsed_url = urlparse(chapter_url)
        url_path = parsed_url.path  # ä¾‹å¦‚: /novel/41934/123.html æˆ– /book/41934/123.html

        # ä»è·¯å¾„ä¸­æå–æ‰€æœ‰æ•°å­—åºåˆ—
        numbers = re.findall(r'\d+', url_path)

        book_id = ''
        chapter_id = ''

        if len(numbers) >= 2:
            # æ ‡å‡†æƒ…å†µ: è‡³å°‘æœ‰ä¸¤ä¸ªæ•°å­—ï¼Œç¬¬ä¸€ä¸ªæ˜¯book_idï¼Œç¬¬äºŒä¸ªæ˜¯chapter_id
            book_id = numbers[0]
            chapter_id = numbers[1]
        elif len(numbers) == 1:
            # åªæœ‰ä¸€ä¸ªæ•°å­—ï¼Œä½œä¸ºchapter_idï¼Œbook_idä½¿ç”¨self.book_id
            chapter_id = numbers[0]
            book_id = self.book_id or ''
        else:
            logger.warning(f"âš ï¸  æ— æ³•ä»URLæå–ID: {chapter_url}")
            return None

        # ä½¿ç”¨ url_templates.chapter_content_page æ„å»ºURL
        try:
            next_url = self.config_manager.build_url(
                'chapter_content_page',
                book_id=book_id,
                chapter_id=chapter_id,
                page=page
            )
            logger.debug(f"ğŸ“„ æ„å»ºç¿»é¡µURL: {next_url} (book_id={book_id}, chapter_id={chapter_id}, page={page})")
            return next_url
        except Exception as e:
            logger.error(f"âŒ æ„å»ºç¿»é¡µURLå¤±è´¥: {e}, chapter_url: {chapter_url}")
            return None

    def _parse_chapters_from_page(self, html: str, chapter_list_config: Dict) -> List[Dict]:
        """ä»é¡µé¢è§£æç« èŠ‚åˆ—è¡¨"""
        chapters = []

        # éªŒè¯é…ç½®ç±»å‹
        if not isinstance(chapter_list_config, dict):
            raise TypeError(f"chapter_list_config åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(chapter_list_config).__name__}")

        # è·å–ç« èŠ‚é¡¹é…ç½®
        items_config = chapter_list_config.get('items')
        title_config = chapter_list_config.get('title')
        url_config = chapter_list_config.get('url')

        # éªŒè¯å¿…éœ€å­—æ®µ
        if not items_config:
            raise ValueError("chapter_list_config ç¼ºå°‘ 'items' å­—æ®µ")
        if not title_config:
            raise ValueError("chapter_list_config ç¼ºå°‘ 'title' å­—æ®µ")
        if not url_config:
            raise ValueError("chapter_list_config ç¼ºå°‘ 'url' å­—æ®µ")

        # éªŒè¯å­—æ®µç±»å‹
        if not isinstance(items_config, dict):
            raise TypeError(f"items é…ç½®åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(items_config).__name__}")
        if not isinstance(title_config, dict):
            raise TypeError(f"title é…ç½®åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(title_config).__name__}")
        if not isinstance(url_config, dict):
            raise TypeError(f"url é…ç½®åº”ä¸ºå­—å…¸ç±»å‹ï¼Œå®é™…ä¸º {type(url_config).__name__}")

        # å…ˆè·å–æ‰€æœ‰ç« èŠ‚é¡¹
        root = Selector(text=html)
        items_xpath = items_config.get('expression', '')
        if not items_xpath:
            raise ValueError("items é…ç½®ç¼ºå°‘ 'expression' å­—æ®µ")

        chapter_items = root.xpath(items_xpath)

        for item in chapter_items:
            try:
                # è§£ææ ‡é¢˜
                title_expr = title_config.get('expression', '')
                if not title_expr:
                    continue
                title = item.xpath(title_expr).get()

                # è§£æURL
                url_expr = url_config.get('expression', '')
                if not url_expr:
                    continue
                url = item.xpath(url_expr).get()

                if title and url:
                    # åå¤„ç† - title
                    if title_config.get('process'):
                        title = self.parser.apply_post_process(title, title_config['process'])

                    # åå¤„ç† - url
                    if url_config.get('process'):
                        url = self.parser.apply_post_process(url, url_config['process'])

                    # æ„å»ºå®Œæ•´URL
                    chapter_url = urljoin(self.base_url, url)

                    chapters.append({
                        'title': title,
                        'url': chapter_url,
                        'content': ''
                    })
            except Exception as e:
                logger.warning(f"âš ï¸  è§£æç« èŠ‚é¡¹å¤±è´¥: {e}")
                continue

        return chapters

    def can_convert_to_int(self, s):
        try:
            return int(float(s))
        except ValueError:
            # è½¬æ¢å¤±è´¥ï¼Œä¸æ˜¯æ•°å­—
            return 0

    def _extract_max_pages_from_html(self, html: str, max_page_xpath_config: Dict, max_pages_manual: int) -> int:
        """
        ä»HTMLé¡µé¢ä¸­æå–æœ€å¤§é¡µæ•°
        :param html: HTMLå†…å®¹
        :param max_page_xpath_config: xpathé…ç½®
        :param max_pages_manual: æ‰‹åŠ¨é…ç½®çš„æœ€å¤§é¡µæ•°ï¼ˆä½œä¸ºé»˜è®¤å€¼ï¼‰
        :return: æå–åˆ°çš„æœ€å¤§é¡µæ•°ï¼ˆå¦‚æœå¤±è´¥åˆ™è¿”å›max_pages_manualï¼‰
        """
        if not max_page_xpath_config:
            return max_pages_manual

        try:
            max_page_from_xpath = self.parser.parse_with_config(html, max_page_xpath_config)
            if max_page_from_xpath:
                # å®‰å…¨åœ°è½¬æ¢ä¸ºæ•´æ•°
                max_page_str = str(max_page_from_xpath).strip()
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                if isinstance(max_page_from_xpath, list) and max_page_from_xpath:
                    max_page_str = str(max_page_from_xpath[0]).strip()

                # å°è¯•è½¬æ¢ä¸ºæ•´æ•°
                max_page_extracted = self.can_convert_to_int(max_page_str)
                if max_page_extracted:
                    # å–xpathæå–çš„æœ€å¤§é¡µå’Œæ‰‹åŠ¨é…ç½®çš„æœ€å¤§é¡µä¸­çš„è¾ƒå¤§
                    logger.info(f"âœ… ä»é¡µé¢æå–å®é™…é¡µæ•°: {max_page_extracted}")
                    return max_page_extracted
                else:
                    logger.warning(f"âš ï¸  æå–çš„æœ€å¤§é¡µæ•°æ ¼å¼æ— æ•ˆ: '{max_page_str}', ä½¿ç”¨æ‰‹åŠ¨é…ç½®çš„å€¼: {max_pages_manual}")
        except Exception as e:
            logger.warning(f"âš ï¸  æå–æœ€å¤§é¡µæ•°å¤±è´¥: {e}, ä½¿ç”¨æ‰‹åŠ¨é…ç½®çš„å€¼: {max_pages_manual}")

        return max_pages_manual

    def download_chapter_content(self, chapter_url: str) -> str:
        """
        ä¸‹è½½ç« èŠ‚å†…å®¹ï¼ˆæ”¯æŒå¤šé¡µï¼‰
        :param chapter_url: ç« èŠ‚URL
        :return: å®Œæ•´å†…å®¹
        """
        all_content = []
        current_url = chapter_url
        page_num = 1

        parsers = self.config_manager.get_parsers()
        chapter_content_config = parsers.get('chapter_content', {})

        content_config = chapter_content_config.get('content', {})
        next_page_config = chapter_content_config.get('next_page', {}) or chapter_content_config.get('pagination', {})
        clean_config = chapter_content_config.get('clean', [])

        # è·å–æœ€å¤§é¡µæ•°ï¼šä¼˜å…ˆä»next_pageé…ç½®è¯»å–ï¼Œå…¼å®¹æ—§é…ç½®
        max_pages_manual = next_page_config.get('max_pages_manual') or chapter_content_config.get('max_pages', 5)
        max_page_xpath_config = next_page_config.get('max_page_xpath')

        # åˆå§‹åŒ–æœ€å¤§é¡µæ•°ï¼ˆé»˜è®¤ä½¿ç”¨æ‰‹åŠ¨é…ç½®çš„å€¼ï¼‰
        max_pages = max_pages_manual
        duplicate_page_count = 0  # è®°å½•å†…å®¹é‡å¤æ•°
        while current_url and page_num <= max_pages:
            html = self.fetcher.get_page(current_url,
                                         max_retries=self.config_manager.get_max_retries())
            if not html:
                logger.warning(f"âš ï¸  ç¬¬{page_num}é¡µè·å–å¤±è´¥")
                break

            # ç¬¬ä¸€é¡µæ—¶å°è¯•ä»é¡µé¢æå–æœ€å¤§é¡µæ•°
            if page_num == 1:
                max_pages = self._extract_max_pages_from_html(html, max_page_xpath_config, max_pages_manual)

            # è§£æå†…å®¹
            content = self.parser.parse_with_config(html, content_config)
            if content:
                if isinstance(content, list):
                    content = '\n'.join([str(c).strip() for c in content if str(c).strip()])

                # æ£€æµ‹è¿ç»­é‡å¤å†…å®¹ï¼ˆä¸ä¸Šä¸€é¡µæ¯”è¾ƒï¼‰
                if all_content and content == all_content[-1]:
                    duplicate_page_count += 1
                    logger.info(f"â„¹ï¸  ç¬¬{page_num}é¡µå†…å®¹ä¸ä¸Šä¸€é¡µé‡å¤ (è¿ç»­{duplicate_page_count}æ¬¡)")
                    if duplicate_page_count >= 3:
                        logger.info(f"âš ï¸  è¿ç»­3é¡µå†…å®¹é‡å¤ï¼Œåœæ­¢ç¿»é¡µ")
                        break
                else:
                    # å†…å®¹ä¸é‡å¤ï¼Œé‡ç½®è®¡æ•°å¹¶æ·»åŠ 
                    if duplicate_page_count > 0:
                        logger.info(f"âœ… ç¬¬{page_num}é¡µå†…å®¹æ­£å¸¸ï¼Œé‡ç½®é‡å¤è®¡æ•°")
                    duplicate_page_count = 0
                    all_content.append(content)

            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            if next_page_config and next_page_config.get('enabled', False):
                # ä½¿ç”¨ url_templates.chapter_content_page æ„å»ºä¸‹ä¸€é¡µURL
                next_url = self._build_content_next_page_url(
                    chapter_url, page_num + 1, next_page_config
                )

                if next_url and next_url != current_url:
                    current_url = next_url
                    page_num += 1
                else:
                    break
            else:
                break

        # åˆå¹¶å†…å®¹
        final_content = '\n\n'.join(all_content) if all_content else ''

        # æ¸…ç†å†…å®¹
        if clean_config:
            for clean_rule in clean_config:
                final_content = self.parser.apply_post_process(final_content, [clean_rule])

        return final_content

    def download_and_save_chapter(self, index: int) -> bool:
        """ä¸‹è½½å¹¶ä¿å­˜å•ä¸ªç« èŠ‚"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
        if self._check_stop():
            self._log('WARNING', 'âš ï¸  æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢ä¸‹è½½')
            return False

        chapter = self.chapters[index]
        chapter_url = chapter['url']
        chapter_title = chapter['title']

        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        if self.is_chapter_downloaded(chapter_url):
            with self.progress_lock:
                self.skipped_count += 1
                self.completed_count += 1
                progress = (self.completed_count / len(self.chapters)) * 100
                msg = f"â­ï¸  [{self.completed_count}/{len(self.chapters)}] {chapter_title} (å·²ä¸‹è½½,è·³è¿‡) - è¿›åº¦: {progress:.1f}%"
                self._log('INFO', msg)
                # æ›´æ–°è¿›åº¦
                self._update_progress(
                    total=len(self.chapters),
                    completed=self.completed_count,
                    failed=len(self.redis_cli.smembers(self.redis_failed_key)) if self.redis_cli else 0,
                    current=chapter_title
                )
            return True

        # ä¸‹è½½å†…å®¹
        content = self.download_chapter_content(chapter_url)
        chapter['content'] = content

        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
        if not content or len(content.strip()) == 0:
            self._log('ERROR', f"âŒ {chapter_title} å†…å®¹ä¸ºç©º")
            self.mark_chapter_failed(chapter_url)
            with self.progress_lock:
                self.completed_count += 1
                # æ›´æ–°è¿›åº¦
                self._update_progress(
                    total=len(self.chapters),
                    completed=self.completed_count,
                    failed=len(self.redis_cli.smembers(self.redis_failed_key)) if self.redis_cli else 0,
                    current=chapter_title
                )
            return False

        # ä¿å­˜åˆ°æ•°æ®åº“
        download_success = False
        db = NovelDatabase(**DB_CONFIG, silent=True)
        if db.connect():
            try:
                db.insert_chapter(
                    self.novel_id,
                    index + 1,
                    chapter_title,
                    content,
                    chapter_url
                )
                db.close()
                download_success = True
            except Exception as e:
                self._log('ERROR', f"âŒ {chapter_title} æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
                db.close()
                download_success = False

        # æ›´æ–°Redisè®°å½•
        if download_success:
            self.mark_chapter_success(chapter_url)
            status_icon = "âœ…"
        else:
            self.mark_chapter_failed(chapter_url)
            status_icon = "âŒ"

        # æ›´æ–°è¿›åº¦
        with self.progress_lock:
            self.completed_count += 1
            progress = (self.completed_count / len(self.chapters)) * 100
            msg = f"{status_icon} [{self.completed_count}/{len(self.chapters)}] {chapter_title} ({len(content)} å­—) - è¿›åº¦: {progress:.1f}%"
            self._log('INFO' if download_success else 'ERROR', msg)

            # è°ƒç”¨è¿›åº¦å›è°ƒ
            self._update_progress(
                total=len(self.chapters),
                completed=self.completed_count,
                failed=len(self.redis_cli.smembers(self.redis_failed_key)) if self.redis_cli else 0,
                current=chapter_title
            )

        # å»¶è¿Ÿ
        delay = self.config_manager.get_delay()
        time.sleep(delay)

        return download_success

    def download_all_chapters(self, retry_failed: bool = False) -> bool:
        """
        å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½æ‰€æœ‰ç« èŠ‚
        :param retry_failed: æ˜¯å¦é‡è¯•å¤±è´¥çš„ç« èŠ‚
        :return: æ˜¯å¦æˆåŠŸ
        """
        # æ˜¾ç¤ºç»Ÿè®¡
        success_count, failed_count = self.get_download_stats()
        logger.info(f"ğŸ“Š Redisç»Ÿè®¡: å·²æˆåŠŸ {success_count} ç« ï¼Œå¤±è´¥ {failed_count} ç« ")

        if retry_failed and failed_count > 0:
            logger.info(f"ğŸ”„ å‡†å¤‡é‡è¯• {failed_count} ä¸ªå¤±è´¥çš„ç« èŠ‚")
            self.clear_failed_records()

        # è¿æ¥æ•°æ®åº“
        if not self.db.connect():
            logger.error("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
            return False

        # æ£€æŸ¥å°è¯´æ˜¯å¦å·²å­˜åœ¨
        existing_novel = self.db.get_novel_by_url(self.start_url)
        if existing_novel:
            self.novel_id = existing_novel['id']
            logger.info(f"ğŸ“š å°è¯´å·²å­˜åœ¨ (ID: {self.novel_id})ï¼Œå°†æ›´æ–°ç« èŠ‚\n")
        else:
            # æ’å…¥å°è¯´ä¿¡æ¯
            self.novel_id = self.db.insert_novel(
                self.novel_info.get('title'),
                self.novel_info.get('author', 'æœªçŸ¥'),
                self.start_url,
                cover_url=self.novel_info.get('cover_url', ''),
                site_name=self.site_name
            )
            if not self.novel_id:
                logger.error("âŒ ä¿å­˜å°è¯´ä¿¡æ¯å¤±è´¥")
                return False
            logger.info(f"âœ… å°è¯´ä¿¡æ¯å·²ä¿å­˜ (ID: {self.novel_id})\n")

        self.db.close()

        # å¤šçº¿ç¨‹ä¸‹è½½
        logger.info("=" * 60)
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘ä¸‹è½½ç« èŠ‚å†…å®¹ (çº¿ç¨‹æ•°: {self.max_workers})")
        logger.info("=" * 60)

        self.completed_count = 0
        self.skipped_count = 0
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.download_and_save_chapter, i): i
                       for i in range(len(self.chapters))}

            for future in as_completed(futures):
                index = futures[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"âŒ ç« èŠ‚ {index + 1} ä¸‹è½½å¤±è´¥: {e}")

        elapsed_time = time.time() - start_time

        # æ›´æ–°ç»Ÿè®¡
        if self.db.connect():
            self.db.update_novel_stats(self.novel_id)
            self.db.close()

        final_success, final_failed = self.get_download_stats()
        new_downloads = self.completed_count - self.skipped_count

        logger.info("\n" + "=" * 60)
        logger.info(f"âœ… æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆï¼")
        logger.info(f"   æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"   æ€»ç« èŠ‚: {len(self.chapters)}")
        logger.info(f"   æ–°ä¸‹è½½: {new_downloads} ç« ")
        logger.info(f"   è·³è¿‡(å·²ä¸‹è½½): {self.skipped_count} ç« ")
        logger.info(f"   æˆåŠŸ: {final_success} ç« ")
        logger.info(f"   å¤±è´¥: {final_failed} ç« ")
        logger.info("=" * 60)

        return True

    def retry_failed_chapters(self) -> bool:
        """åªé‡è¯•å¤±è´¥çš„ç« èŠ‚"""
        logger.info("=" * 60)
        logger.info("ğŸ”„ å¼€å§‹é‡è¯•å¤±è´¥çš„ç« èŠ‚")
        logger.info("=" * 60)

        try:
            # è·å–å¤±è´¥ç« èŠ‚URL
            failed_urls = self.redis_cli.smembers(self.redis_failed_key)
            if not failed_urls:
                logger.info("âœ… æ²¡æœ‰å¤±è´¥çš„ç« èŠ‚éœ€è¦é‡è¯•")
                return True

            failed_urls = [url.decode('utf-8') if isinstance(url, bytes) else url for url in failed_urls]
            logger.info(f"ğŸ“‹ å…±æœ‰ {len(failed_urls)} ä¸ªå¤±è´¥ç« èŠ‚éœ€è¦é‡è¯•")

            # è§£æç« èŠ‚åˆ—è¡¨
            if not self.parse_chapter_list():
                logger.error("âŒ è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥")
                return False

            # xè·å–å°è¯´ID
            if not self.db.connect():
                logger.error("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
                return False

            existing_novel = self.db.get_novel_by_url(self.start_url)
            if existing_novel:
                self.novel_id = existing_novel['id']
            else:
                logger.error("âŒ å°è¯´ä¸å­˜åœ¨")
                self.db.close()
                return False

            self.db.close()

            # ç­›é€‰éœ€è¦é‡è¯•çš„ç« èŠ‚
            retry_chapters = []
            for idx, chapter in enumerate(self.chapters):
                if chapter['url'] in failed_urls:
                    retry_chapters.append((idx, chapter))

            logger.info(f"ğŸ¯ åŒ¹é…åˆ° {len(retry_chapters)} ä¸ªç« èŠ‚éœ€è¦é‡è¯•")

            if not retry_chapters:
                logger.info("âœ… æ²¡æœ‰åŒ¹é…çš„ç« èŠ‚éœ€è¦é‡è¯•")
                return True

            # æ¸…é™¤å¤±è´¥è®°å½•
            self.clear_failed_records()

            # å¤šçº¿ç¨‹é‡è¯•
            self.completed_count = 0
            self.skipped_count = 0
            start_time = time.time()

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {executor.submit(self.download_and_save_chapter, idx): idx
                           for idx, chapter in retry_chapters}

                for future in as_completed(futures):
                    index = futures[future]
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"âŒ ç« èŠ‚ {index + 1} é‡è¯•å¤±è´¥: {e}")

            elapsed_time = time.time() - start_time
            final_success, final_failed = self.get_download_stats()

            logger.info("\n" + "=" * 60)
            logger.info(f"âœ… é‡è¯•å®Œæˆï¼")
            logger.info(f"   è€—æ—¶: {elapsed_time:.2f} ç§’")
            logger.info(f"   é‡è¯•ç« èŠ‚: {len(retry_chapters)}")
            logger.info(f"   å½“å‰æˆåŠŸ: {final_success} ç« ")
            logger.info(f"   å½“å‰å¤±è´¥: {final_failed} ç« ")
            logger.info("=" * 60)

            return True

        except Exception as e:
            logger.error(f"âŒ é‡è¯•å¤±è´¥ç« èŠ‚æ—¶å‡ºé”™: {e}")
            return False

    def print_summary(self):
        """æ‰“å°ä¸‹è½½æ‘˜è¦"""
        success_count, failed_count = self.get_download_stats()

        logger.info(f"\n{'=' * 60}")
        logger.info(f"âœ… ä¸‹è½½å®Œæˆï¼")
        logger.info(f"{'=' * 60}")
        logger.info(f"å°è¯´ID: {self.novel_id}")
        logger.info(f"å°è¯´åç§°: {self.novel_info.get('title')}")
        logger.info(f"ä½œè€…: {self.novel_info.get('author', 'æœªçŸ¥')}")
        logger.info(f"ç« èŠ‚æ€»æ•°: {len(self.chapters)}")
        logger.info(f"æˆåŠŸç« èŠ‚: {success_count}")
        logger.info(f"å¤±è´¥ç« èŠ‚: {failed_count}")

        total_words = sum(len(ch['content']) for ch in self.chapters)
        logger.info(f"æ€»å­—æ•°: {total_words:,} å­—")
        logger.info(f"{'=' * 60}")

    def run(self):
        """
        è¿è¡Œçˆ¬è™«ï¼ˆå®Œæ•´æµç¨‹ï¼‰
        1. è§£æç« èŠ‚åˆ—è¡¨
        2. ä¸‹è½½æ‰€æœ‰ç« èŠ‚
        3. æ‰“å°æ‘˜è¦
        """
        try:
            self._log('INFO', "=" * 60)
            self._log('INFO', f"ğŸš€ å¼€å§‹è¿è¡Œçˆ¬è™«: {self.site_name}")
            self._log('INFO', f"ğŸ“– ä¹¦ç±ID: {self.book_id}")
            self._log('INFO', "=" * 60)

            # 1. è§£æç« èŠ‚åˆ—è¡¨
            if not self.parse_chapter_list():
                self._log('ERROR', "âŒ è§£æç« èŠ‚åˆ—è¡¨å¤±è´¥")
                return False

            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self._check_stop():
                self._log('WARNING', 'âš ï¸  ä»»åŠ¡è¢«åœæ­¢')
                return False

            # 2. ä¸‹è½½æ‰€æœ‰ç« èŠ‚
            if not self.download_all_chapters():
                self._log('ERROR', "âŒ ä¸‹è½½ç« èŠ‚å¤±è´¥")
                return False

            # 3. æ‰“å°æ‘˜è¦
            self.print_summary()

            self._log('INFO', "=" * 60)
            self._log('SUCCESS', "âœ… çˆ¬è™«è¿è¡Œå®Œæˆï¼")
            self._log('INFO', "=" * 60)

            return True

        except Exception as e:
            self._log('ERROR', f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            raise

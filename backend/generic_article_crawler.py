#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨æ–‡ç« /æ–°é—»çˆ¬è™«æ¡†æ¶
ä¸“é—¨ç”¨äºå¤„ç†æ–°é—»ã€æ–‡ç« ã€åšå®¢ç­‰éå°è¯´ç±»å†…å®¹
ç‰¹ç‚¹ï¼š
- ä¸éœ€è¦book_idï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´URL
- æ”¯æŒæ–‡ç« åˆ—è¡¨é¡µ+è¯¦æƒ…é¡µçš„çˆ¬å–æ¨¡å¼
- çµæ´»çš„å†…å®¹æå–è§„åˆ™
"""
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from threading import Lock
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse
from datetime import datetime

from loguru import logger
from redis import Redis
from scrapy import Selector

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from shared.utils.config import DB_CONFIG, REDIS_CONFIG
from backend.models.database import NovelDatabase
from shared.utils.proxy_utils import ProxyUtils
from backend.config_manager import ConfigManager
from backend.parser import HtmlParser
from backend.content_fetcher import ContentFetcher

# ä»é…ç½®è¯»å–Redisè¿æ¥ä¿¡æ¯
REDIS_URL = f"redis://{REDIS_CONFIG['host']}:{REDIS_CONFIG['port']}/{REDIS_CONFIG['db']}"
redis_cli = Redis.from_url(REDIS_URL)


class GenericArticleCrawler:
    """é€šç”¨æ–‡ç« /æ–°é—»çˆ¬è™«"""

    def __init__(self, config_file: str, start_url: str, max_workers: int = 5, use_proxy: bool = False,
                 progress_callback=None, log_callback=None, stop_flag=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        :param start_url: èµ·å§‹URLï¼ˆåˆ—è¡¨é¡µæˆ–å•ç¯‡æ–‡ç« ï¼‰
        :param max_workers: å¹¶å‘çº¿ç¨‹æ•°
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        :param progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        :param log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
        :param stop_flag: åœæ­¢æ ‡å¿—
        """
        self.start_url = start_url
        self.max_workers = max_workers
        self.use_proxy = use_proxy

        # å›è°ƒå‡½æ•°
        self.progress_callback = progress_callback
        self.log_callback = log_callback
        self.stop_flag = stop_flag

        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager(config_file)
        site_info = self.config_manager.get_site_info()

        self.site_name = site_info.get('name')
        self.base_url = site_info.get('base_url')

        # åˆå§‹åŒ–HTMLè§£æå™¨
        self.parser = HtmlParser(self.base_url)

        # åˆå§‹åŒ–ä»£ç†å·¥å…·
        proxy_utils = None
        if use_proxy:
            proxy_utils = ProxyUtils()
            self._log('INFO', "âœ… å·²å¯ç”¨ä»£ç†")

        # åˆå§‹åŒ–å†…å®¹è·å–å™¨
        self.fetcher = ContentFetcher(
            headers=self.config_manager.get_headers(),
            timeout=self.config_manager.get_timeout(),
            encoding=self.config_manager.get_encoding(),
            proxy_utils=proxy_utils
        )

        # æ•°æ®å­˜å‚¨
        self.articles = []  # æ–‡ç« åˆ—è¡¨ï¼ˆç±»ä¼¼chaptersï¼‰
        self.site_info_data = {}  # ç½‘ç«™ä¿¡æ¯ï¼ˆç±»ä¼¼novel_infoï¼‰
        self.novel_id = None  # ä¸ºäº†å…¼å®¹æ•°æ®åº“ç»“æ„ï¼Œä»ä½¿ç”¨novelè¡¨å­˜å‚¨

        # ä½¿ç”¨å•ä¾‹æ¨¡å¼è·å–æ•°æ®åº“è¿æ¥
        from backend.models.database import get_database
        self.db = get_database(**DB_CONFIG, silent=True)

        # å¹¶å‘é…ç½®
        self.progress_lock = Lock()
        self.completed_count = 0
        self.skipped_count = 0
        self.failed_count = 0

        # Redisé…ç½®
        self.redis_cli = redis_cli
        # ä½¿ç”¨URLçš„hashä½œä¸ºå”¯ä¸€æ ‡è¯†
        self.url_hash = str(hash(start_url))
        self.redis_success_key = f"article:success:{self.site_name}:{self.url_hash}"
        self.redis_failed_key = f"article:failed:{self.site_name}:{self.url_hash}"

        self._log('INFO', f"ğŸŒ ç½‘ç«™: {self.site_name}")
        self._log('INFO', f"ğŸ”— èµ·å§‹URL: {start_url}")

    def _log(self, level: str, message: str):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
        logger_func = {
            'INFO': logger.info,
            'WARNING': logger.warning,
            'ERROR': logger.error,
            'SUCCESS': logger.success
        }.get(level, logger.info)

        logger_func(message)

        if self.log_callback:
            try:
                self.log_callback(level, message)
            except Exception as e:
                logger.error(f"æ—¥å¿—å›è°ƒå¤±è´¥: {e}")

    def _update_progress(self, stage='downloading', detail='', **kwargs):
        """æ›´æ–°è¿›åº¦"""
        if self.progress_callback:
            try:
                progress_data = {
                    'stage': stage,
                    'detail': detail,
                    'total': kwargs.get('total', len(self.articles) if self.articles else 0),
                    'completed': kwargs.get('completed', self.completed_count),
                    'failed': kwargs.get('failed', self.failed_count),
                    'current': kwargs.get('current', ''),
                }
                progress_data.update(kwargs)
                self.progress_callback(**progress_data)
            except Exception as e:
                logger.error(f"è¿›åº¦å›è°ƒå¤±è´¥: {e}")

    def _check_stop(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢"""
        if self.stop_flag and self.stop_flag.is_set():
            return True
        return False

    def is_article_downloaded(self, article_url: str) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²ä¸‹è½½"""
        try:
            return self.redis_cli.sismember(self.redis_success_key, article_url)
        except Exception as e:
            logger.warning(f"âš ï¸  Redisæ£€æŸ¥å¤±è´¥: {e}")
            return False

    def mark_article_success(self, article_url: str):
        """æ ‡è®°æ–‡ç« ä¸‹è½½æˆåŠŸ"""
        try:
            self.redis_cli.sadd(self.redis_success_key, article_url)
            self.redis_cli.srem(self.redis_failed_key, article_url)
            self.redis_cli.expire(self.redis_success_key, 30 * 24 * 3600)
        except Exception as e:
            logger.warning(f"âš ï¸  Redisè®°å½•æˆåŠŸå¤±è´¥: {e}")

    def mark_article_failed(self, article_url: str):
        """æ ‡è®°æ–‡ç« ä¸‹è½½å¤±è´¥"""
        try:
            self.redis_cli.sadd(self.redis_failed_key, article_url)
            self.redis_cli.expire(self.redis_failed_key, 7 * 24 * 3600)
            self.failed_count += 1
        except Exception as e:
            logger.warning(f"âš ï¸  Redisè®°å½•å¤±è´¥å¤±è´¥: {e}")

    def parse_site_info(self, html: str) -> Dict:
        """è§£æç½‘ç«™/é¡µé¢ä¿¡æ¯ï¼ˆç±»ä¼¼parse_novel_infoï¼‰"""
        site_data = {}
        parsers = self.config_manager.get_parsers().get('novel_info', {})

        if not isinstance(parsers, dict):
            return site_data

        for field, parser_config in parsers.items():
            if field.startswith('_'):
                continue

            try:
                value = self.parser.parse_with_config(html, parser_config)
                site_data[field] = value
            except Exception as e:
                logger.warning(f"âš ï¸  è§£æå­—æ®µ {field} å¤±è´¥: {e}")
                site_data[field] = None

        return site_data

    def parse_article_list(self) -> bool:
        """è§£ææ–‡ç« åˆ—è¡¨"""
        self._log('INFO', f"ğŸ“– å¼€å§‹è·å–æ–‡ç« åˆ—è¡¨...")
        self._update_progress(stage='parsing_list', detail='æ­£åœ¨è·å–åˆ—è¡¨é¡µ...')

        try:
            html = self.fetcher.get_page(self.start_url)
            if not html:
                self._log('ERROR', f"âŒ è·å–åˆ—è¡¨é¡µå¤±è´¥: {self.start_url}")
                return False

            # è§£æç½‘ç«™/é¡µé¢ä¿¡æ¯
            self.site_info_data = self.parse_site_info(html)
            self._log('INFO', f"ğŸ“Š ç½‘ç«™ä¿¡æ¯: {self.site_info_data}")

            # è·å–chapter_listé…ç½®ï¼ˆå¤ç”¨é…ç½®ç»“æ„ï¼‰
            chapter_list_config = self.config_manager.get_parsers().get('chapter_list', {})
            if not chapter_list_config:
                self._log('ERROR', "âŒ æœªé…ç½®chapter_listè§£æè§„åˆ™")
                return False

            # è§£ææ–‡ç« åˆ—è¡¨
            self.articles = self._parse_article_items(html, chapter_list_config)
            
            if not self.articles:
                self._log('WARNING', "âš ï¸  æœªæ‰¾åˆ°æ–‡ç« ")
                return False

            self._log('SUCCESS', f"âœ… å…±æ‰¾åˆ° {len(self.articles)} ç¯‡æ–‡ç« ")
            self._update_progress(
                stage='parsing_list',
                detail=f'æ‰¾åˆ° {len(self.articles)} ç¯‡æ–‡ç« ',
                total=len(self.articles)
            )

            return True

        except Exception as e:
            self._log('ERROR', f"âŒ è§£ææ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
            logger.exception(e)
            return False

    def _parse_article_items(self, html: str, config: Dict) -> List[Dict]:
        """è§£ææ–‡ç« åˆ—è¡¨é¡¹"""
        articles = []
        
        try:
            # è·å–itemsé…ç½®
            items_config = config.get('items', {})
            if not items_config:
                self._log('WARNING', "âš ï¸  æœªé…ç½®itemsè§„åˆ™")
                return articles

            # è§£ææ–‡ç« å®¹å™¨
            items_html = self.parser.parse_with_config(html, items_config)
            if not items_html or not isinstance(items_html, list):
                self._log('WARNING', "âš ï¸  æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨å®¹å™¨")
                return articles

            self._log('INFO', f"ğŸ“¦ æ‰¾åˆ° {len(items_html)} ä¸ªæ–‡ç« å®¹å™¨")

            # éå†æ¯ä¸ªæ–‡ç« å®¹å™¨
            for idx, item_html in enumerate(items_html, 1):
                try:
                    # è§£ææ ‡é¢˜
                    title_config = config.get('title', {})
                    title = self.parser.parse_with_config(item_html, title_config) if title_config else f"æ–‡ç« {idx}"

                    # è§£æURL
                    url_config = config.get('url', {})
                    url = self.parser.parse_with_config(item_html, url_config) if url_config else ''

                    if url:
                        # è½¬æ¢ä¸ºå®Œæ•´URL
                        url = urljoin(self.start_url, url)
                        
                        articles.append({
                            'num': idx,
                            'title': title or f"æ–‡ç« {idx}",
                            'url': url
                        })
                        
                        if idx <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ª
                            self._log('INFO', f"  ğŸ“„ [{idx}] {title}")

                except Exception as e:
                    logger.warning(f"âš ï¸  è§£æç¬¬ {idx} ä¸ªæ–‡ç« å¤±è´¥: {e}")
                    continue

            if len(articles) > 3:
                self._log('INFO', f"  ... è¿˜æœ‰ {len(articles) - 3} ç¯‡æ–‡ç« ")

        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« åˆ—è¡¨é¡¹å¤±è´¥: {e}")
            logger.exception(e)

        return articles

    def parse_article_content(self, article_url: str) -> Optional[str]:
        """è§£æå•ç¯‡æ–‡ç« å†…å®¹"""
        try:
            html = self.fetcher.get_page(article_url)
            if not html:
                return None

            # è·å–contenté…ç½®
            content_config = self.config_manager.get_parsers().get('chapter_content', {})
            if not content_config:
                self._log('ERROR', "âŒ æœªé…ç½®chapter_contentè§£æè§„åˆ™")
                return None

            # è§£æå†…å®¹
            content_parser = content_config.get('content', {})
            if not content_parser:
                self._log('ERROR', "âŒ æœªé…ç½®contentå­—æ®µ")
                return None

            content = self.parser.parse_with_config(html, content_parser)
            
            if not content:
                self._log('WARNING', f"âš ï¸  æœªæå–åˆ°å†…å®¹: {article_url}")
                return None

            return content

        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« å†…å®¹å¤±è´¥ [{article_url}]: {e}")
            return None

    def download_article(self, article: Dict) -> bool:
        """ä¸‹è½½å•ç¯‡æ–‡ç« """
        article_num = article['num']
        title = article['title']
        url = article['url']

        # æ£€æŸ¥æ˜¯å¦å·²åœæ­¢
        if self._check_stop():
            self._log('WARNING', f"â¸ï¸  ç”¨æˆ·åœæ­¢ï¼Œè·³è¿‡: [{article_num}] {title}")
            return False

        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        if self.is_article_downloaded(url):
            with self.progress_lock:
                self.skipped_count += 1
            self._log('INFO', f"â­ï¸  å·²ä¸‹è½½ï¼Œè·³è¿‡: [{article_num}] {title}")
            return True

        try:
            # æ›´æ–°è¿›åº¦
            with self.progress_lock:
                self._update_progress(current=f"[{article_num}] {title}")

            # è§£æå†…å®¹
            content = self.parse_article_content(url)
            if not content:
                with self.progress_lock:
                    self.mark_article_failed(url)
                self._log('ERROR', f"âŒ ä¸‹è½½å¤±è´¥: [{article_num}] {title}")
                return False

            # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¤ç”¨chapterè¡¨ç»“æ„ï¼‰
            success = self.db.save_chapter(
                novel_id=self.novel_id,
                chapter_num=article_num,
                title=title,
                content=content,
                chapter_url=url
            )

            if success:
                with self.progress_lock:
                    self.completed_count += 1
                    self.mark_article_success(url)
                self._log('SUCCESS', f"âœ… [{article_num}] {title}")
                return True
            else:
                with self.progress_lock:
                    self.mark_article_failed(url)
                self._log('ERROR', f"âŒ ä¿å­˜å¤±è´¥: [{article_num}] {title}")
                return False

        except Exception as e:
            with self.progress_lock:
                self.mark_article_failed(url)
            self._log('ERROR', f"âŒ ä¸‹è½½å¼‚å¸¸: [{article_num}] {title} - {e}")
            logger.exception(e)
            return False

    def download_all_articles(self) -> bool:
        """å¹¶å‘ä¸‹è½½æ‰€æœ‰æ–‡ç« """
        if not self.articles:
            self._log('WARNING', "âš ï¸  æ²¡æœ‰æ–‡ç« éœ€è¦ä¸‹è½½")
            return False

        self._log('INFO', f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {len(self.articles)} ç¯‡æ–‡ç« ...")
        self._log('INFO', f"âš¡ å¹¶å‘æ•°: {self.max_workers}")

        start_time = time.time()
        self._update_progress(stage='downloading', detail='æ­£åœ¨ä¸‹è½½æ–‡ç« ...', total=len(self.articles))

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self.download_article, article): article
                for article in self.articles
            }

            for future in as_completed(futures):
                if self._check_stop():
                    self._log('WARNING', "â¸ï¸  ç”¨æˆ·åœæ­¢ä¸‹è½½")
                    executor.shutdown(wait=False, cancel_futures=True)
                    break

                article = futures[future]
                try:
                    future.result()
                except Exception as e:
                    self._log('ERROR', f"âŒ æ–‡ç«  [{article['num']}] ä¸‹è½½å¼‚å¸¸: {e}")

                # æ›´æ–°è¿›åº¦
                with self.progress_lock:
                    self._update_progress(
                        stage='downloading',
                        detail=f'å·²å®Œæˆ {self.completed_count}/{len(self.articles)}',
                        total=len(self.articles),
                        completed=self.completed_count,
                        failed=self.failed_count
                    )

        elapsed = time.time() - start_time
        self._log('SUCCESS', f"â±ï¸  ä¸‹è½½è€—æ—¶: {elapsed:.2f}ç§’")
        self._log('SUCCESS', f"âœ… æˆåŠŸ: {self.completed_count} | â­ï¸  è·³è¿‡: {self.skipped_count} | âŒ å¤±è´¥: {self.failed_count}")

        return self.completed_count > 0

    def save_site_info(self) -> bool:
        """ä¿å­˜ç½‘ç«™/å†…å®¹é›†ä¿¡æ¯åˆ°æ•°æ®åº“ï¼ˆå¤ç”¨novelè¡¨ï¼‰"""
        try:
            # æ„å»º"å°è¯´"ä¿¡æ¯ï¼ˆå®é™…æ˜¯æ–‡ç« é›†ï¼‰
            title = self.site_info_data.get('title') or self.site_name or 'æ–‡ç« é›†åˆ'
            author = self.site_info_data.get('author') or 'æœªçŸ¥'
            description = self.site_info_data.get('description') or f'æ¥è‡ª {self.site_name}'
            
            # ä½¿ç”¨URL hashä½œä¸ºå”¯ä¸€æ ‡è¯†
            novel_url = self.start_url
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self.novel_id = self.db.save_novel(
                title=title,
                author=author,
                description=description,
                novel_url=novel_url,
                cover_url=self.site_info_data.get('cover_url'),
                source=self.site_name,
                category='article'  # æ ‡è®°ä¸ºæ–‡ç« ç±»å‹
            )

            if self.novel_id:
                self._log('SUCCESS', f"âœ… ä¿å­˜ç«™ç‚¹ä¿¡æ¯æˆåŠŸ (ID: {self.novel_id})")
                return True
            else:
                self._log('ERROR', "âŒ ä¿å­˜ç«™ç‚¹ä¿¡æ¯å¤±è´¥")
                return False

        except Exception as e:
            self._log('ERROR', f"âŒ ä¿å­˜ç«™ç‚¹ä¿¡æ¯å¼‚å¸¸: {e}")
            logger.exception(e)
            return False

    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            self._log('INFO', "=" * 60)
            self._log('INFO', f"ğŸš€ å¼€å§‹çˆ¬å– {self.site_name} æ–‡ç« ")
            self._log('INFO', "=" * 60)

            # 1. è§£ææ–‡ç« åˆ—è¡¨
            if not self.parse_article_list():
                self._log('ERROR', "âŒ è§£ææ–‡ç« åˆ—è¡¨å¤±è´¥")
                return

            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self._check_stop():
                self._log('WARNING', "â¸ï¸  ç”¨æˆ·åœæ­¢çˆ¬å–")
                return

            # 2. ä¿å­˜ç«™ç‚¹ä¿¡æ¯
            if not self.save_site_info():
                self._log('ERROR', "âŒ ä¿å­˜ç«™ç‚¹ä¿¡æ¯å¤±è´¥")
                return

            # 3. ä¸‹è½½æ‰€æœ‰æ–‡ç« 
            if not self.download_all_articles():
                self._log('ERROR', "âŒ ä¸‹è½½æ–‡ç« å¤±è´¥")
                return

            # 4. å®Œæˆ
            self._update_progress(
                stage='completed',
                detail='çˆ¬å–å®Œæˆ',
                total=len(self.articles),
                completed=self.completed_count,
                failed=self.failed_count
            )

            self._log('INFO', "=" * 60)
            self._log('SUCCESS', "âœ… çˆ¬å–å®Œæˆï¼")
            self._log('INFO', "=" * 60)

        except KeyboardInterrupt:
            self._log('WARNING', "âš ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        except Exception as e:
            self._log('ERROR', f"âŒ çˆ¬å–å¤±è´¥: {e}")
            logger.exception(e)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='é€šç”¨æ–‡ç« /æ–°é—»çˆ¬è™«')
    parser.add_argument('config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('url', help='èµ·å§‹URLï¼ˆåˆ—è¡¨é¡µæˆ–æ–‡ç« é¡µï¼‰')
    parser.add_argument('-w', '--workers', type=int, default=5, help='å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤5ï¼‰')
    parser.add_argument('--proxy', action='store_true', help='ä½¿ç”¨ä»£ç†')

    args = parser.parse_args()

    crawler = GenericArticleCrawler(
        config_file=args.config,
        start_url=args.url,
        max_workers=args.workers,
        use_proxy=args.proxy
    )

    crawler.run()


if __name__ == '__main__':
    main()


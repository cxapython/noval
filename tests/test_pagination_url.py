#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ç¿»é¡µURLæ„å»ºé€»è¾‘
éªŒè¯ç« èŠ‚åˆ—è¡¨å’Œç« èŠ‚å†…å®¹çš„ç¿»é¡µURLæ˜¯å¦æ­£ç¡®ä½¿ç”¨url_templates
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from loguru import logger
from backend.generic_crawler import GenericNovelCrawler


def test_chapter_list_pagination():
    """æµ‹è¯•ç« èŠ‚åˆ—è¡¨ç¿»é¡µURLæ„å»º"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯•ç« èŠ‚åˆ—è¡¨ç¿»é¡µURL")
    logger.info("=" * 60)
    
    # ä½¿ç”¨ç°æœ‰é…ç½®æ–‡ä»¶
    config_file = str(project_root / "configs" / "config_djks5_v4.json")
    book_id = "41934"
    
    try:
        crawler = GenericNovelCrawler(
            config_file=config_file,
            book_id=book_id,
            max_workers=1,
            use_proxy=False
        )
        
        # æµ‹è¯•æ„å»ºç¬¬2é¡µURL
        page_2_url = crawler._build_pagination_url(page=2)
        logger.info(f"âœ… ç¬¬2é¡µURL: {page_2_url}")
        
        # æµ‹è¯•æ„å»ºç¬¬3é¡µURL
        page_3_url = crawler._build_pagination_url(page=3)
        logger.info(f"âœ… ç¬¬3é¡µURL: {page_3_url}")
        
        # éªŒè¯URLæ ¼å¼
        assert book_id in page_2_url, "URLåº”åŒ…å«book_id"
        assert "2" in page_2_url or "/2/" in page_2_url, "URLåº”åŒ…å«é¡µç 2"
        logger.info("âœ… ç« èŠ‚åˆ—è¡¨ç¿»é¡µURLæ ¼å¼æ­£ç¡®")
        
        return True
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_chapter_content_pagination():
    """æµ‹è¯•ç« èŠ‚å†…å®¹ç¿»é¡µURLæ„å»º"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•ç« èŠ‚å†…å®¹ç¿»é¡µURL")
    logger.info("=" * 60)
    
    # ä½¿ç”¨ç°æœ‰é…ç½®æ–‡ä»¶
    config_file = str(project_root / "configs" / "config_djks5_v4.json")
    book_id = "41934"
    
    try:
        crawler = GenericNovelCrawler(
            config_file=config_file,
            book_id=book_id,
            max_workers=1,
            use_proxy=False
        )
        
        # æµ‹è¯•ä¸åŒæ ¼å¼çš„ç« èŠ‚URL
        test_cases = [
            ("https://m.djks5.com/novel/41934/1.html", "41934", "1"),
            ("https://m.djks5.com/novel/41934/123.html", "41934", "123"),
            ("https://example.com/book/12345/chapter/678.html", "12345", "678"),
        ]
        
        for chapter_url, expected_book_id, expected_chapter_id in test_cases:
            logger.info(f"\næµ‹è¯•ç« èŠ‚URL: {chapter_url}")
            
            # æµ‹è¯•æ„å»ºç¬¬2é¡µURL
            page_2_url = crawler._build_content_next_page_url(chapter_url, page=2)
            logger.info(f"  ç¬¬2é¡µURL: {page_2_url}")
            
            if page_2_url:
                # éªŒè¯URLåŒ…å«å¿…è¦çš„ID
                assert expected_book_id in page_2_url, f"URLåº”åŒ…å«book_id: {expected_book_id}"
                assert expected_chapter_id in page_2_url, f"URLåº”åŒ…å«chapter_id: {expected_chapter_id}"
                assert "2" in page_2_url or "_2" in page_2_url, "URLåº”åŒ…å«é¡µç 2"
                logger.info(f"  âœ… URLæ ¼å¼æ­£ç¡®")
            else:
                logger.warning(f"  âš ï¸  URLæ„å»ºå¤±è´¥")
        
        logger.info("\nâœ… ç« èŠ‚å†…å®¹ç¿»é¡µURLæµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_url_templates_usage():
    """éªŒè¯æ˜¯å¦ä½¿ç”¨äº†url_templates"""
    logger.info("\n" + "=" * 60)
    logger.info("éªŒè¯url_templatesä½¿ç”¨æƒ…å†µ")
    logger.info("=" * 60)
    
    config_file = str(project_root / "configs" / "config_djks5_v4.json")
    book_id = "41934"
    
    try:
        crawler = GenericNovelCrawler(
            config_file=config_file,
            book_id=book_id,
            max_workers=1,
            use_proxy=False
        )
        
        # æ˜¾ç¤ºurl_templatesé…ç½®
        logger.info("å½“å‰url_templatesé…ç½®:")
        for key, value in crawler.url_templates.items():
            logger.info(f"  {key}: {value}")
        
        # æµ‹è¯•åˆ—è¡¨é¡µURL
        list_url = crawler._build_pagination_url(page=2)
        logger.info(f"\nç« èŠ‚åˆ—è¡¨ç¬¬2é¡µURL: {list_url}")
        
        # æµ‹è¯•å†…å®¹é¡µURL
        chapter_url = "https://m.djks5.com/novel/41934/1.html"
        content_url = crawler._build_content_next_page_url(chapter_url, page=2)
        logger.info(f"ç« èŠ‚å†…å®¹ç¬¬2é¡µURL: {content_url}")
        
        logger.info("\nâœ… æ‰€æœ‰URLå‡é€šè¿‡url_templatesæ„å»º")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•ç¿»é¡µURLæ„å»ºé€»è¾‘\n")
    
    results = []
    
    # æµ‹è¯•1: ç« èŠ‚åˆ—è¡¨ç¿»é¡µ
    results.append(test_chapter_list_pagination())
    
    # æµ‹è¯•2: ç« èŠ‚å†…å®¹ç¿»é¡µ
    results.append(test_chapter_content_pagination())
    
    # æµ‹è¯•3: éªŒè¯url_templatesä½¿ç”¨
    results.append(test_url_templates_usage())
    
    # æ€»ç»“
    logger.info("\n" + "=" * 60)
    if all(results):
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        sys.exit(0)
    else:
        logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        sys.exit(1)


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• djks5 ç½‘ç«™çˆ¬è™«å®Œæ•´æµç¨‹
éªŒè¯æ‰€æœ‰åŠŸèƒ½æ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œ
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from backend.generic_crawler import GenericNovelCrawler
from loguru import logger

def test_djks5_workflow():
    """
    æµ‹è¯• djks5 å®Œæ•´å·¥ä½œæµç¨‹
    
    æµç¨‹ï¼š
    1. è®¿é—®ä¹¦ç±è¯¦æƒ…é¡µï¼ˆhttps://www.djks5.com/book/389253.htmlï¼‰
    2. è§£æå°è¯´ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å°é¢ï¼‰
    3. è§£æç¬¬ä¸€é¡µç« èŠ‚åˆ—è¡¨ï¼ˆä½¿ç”¨å—æ¦‚å¿µï¼‰
    4. è§£æç« èŠ‚åˆ—è¡¨æ€»é¡µæ•°
    5. ç¿»é¡µè·å–æ‰€æœ‰ç« èŠ‚
    6. ä¸‹è½½ç« èŠ‚å†…å®¹ï¼ˆæ”¯æŒèŠ‚çš„ç¿»é¡µï¼‰
    """
    logger.info("=" * 80)
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯• djks5 ç½‘ç«™çˆ¬è™«å®Œæ•´æµç¨‹")
    logger.info("=" * 80)
    
    # é…ç½®æ–‡ä»¶
    config_file = project_root / 'configs' / 'config_djks5.json'
    book_id = '389253'
    
    logger.info(f"\nğŸ“‹ æµ‹è¯•é…ç½®:")
    logger.info(f"   é…ç½®æ–‡ä»¶: {config_file}")
    logger.info(f"   ä¹¦ç±ID: {book_id}")
    logger.info(f"   ç›®æ ‡URL: https://www.djks5.com/book/{book_id}")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = GenericNovelCrawler(
        config_file=str(config_file),
        book_id=book_id,
        max_workers=3,  # æµ‹è¯•ç”¨å°å¹¶å‘
        use_proxy=False
    )
    
    # æµ‹è¯•æ­¥éª¤1: è§£æç« èŠ‚åˆ—è¡¨
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ æ­¥éª¤ 1: è§£æç« èŠ‚åˆ—è¡¨ï¼ˆåŒ…å«å°è¯´ä¿¡æ¯ã€ç¿»é¡µç­‰ï¼‰")
    logger.info("=" * 80)
    
    success = crawler.parse_chapter_list()
    
    if not success:
        logger.error("âŒ ç« èŠ‚åˆ—è¡¨è§£æå¤±è´¥")
        return False
    
    # éªŒè¯å°è¯´ä¿¡æ¯
    logger.info("\n" + "-" * 80)
    logger.info("âœ… å°è¯´ä¿¡æ¯è§£æç»“æœ:")
    logger.info("-" * 80)
    logger.info(f"   ğŸ“š æ ‡é¢˜: {crawler.novel_info.get('title')}")
    logger.info(f"   âœï¸  ä½œè€…: {crawler.novel_info.get('author')}")
    logger.info(f"   ğŸ–¼ï¸  å°é¢: {crawler.novel_info.get('cover_url', 'æ— ')}")
    
    # éªŒè¯ç« èŠ‚åˆ—è¡¨
    logger.info("\n" + "-" * 80)
    logger.info("âœ… ç« èŠ‚åˆ—è¡¨è§£æç»“æœ:")
    logger.info("-" * 80)
    logger.info(f"   ğŸ“– æ€»ç« èŠ‚æ•°: {len(crawler.chapters)}")
    
    if len(crawler.chapters) > 0:
        logger.info(f"\n   å‰ 5 ç« é¢„è§ˆ:")
        for i, chapter in enumerate(crawler.chapters[:5]):
            logger.info(f"      {i+1}. {chapter['title']}")
            logger.info(f"         URL: {chapter['url']}")
        
        if len(crawler.chapters) > 5:
            logger.info(f"\n   å 3 ç« é¢„è§ˆ:")
            for i, chapter in enumerate(crawler.chapters[-3:], len(crawler.chapters) - 2):
                logger.info(f"      {i}. {chapter['title']}")
                logger.info(f"         URL: {chapter['url']}")
    
    # æµ‹è¯•æ­¥éª¤2: ä¸‹è½½å•ä¸ªç« èŠ‚å†…å®¹ï¼ˆæµ‹è¯•å†…å®¹ç¿»é¡µï¼‰
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ æ­¥éª¤ 2: æµ‹è¯•ç« èŠ‚å†…å®¹ä¸‹è½½ï¼ˆåŒ…å«èŠ‚çš„ç¿»é¡µï¼‰")
    logger.info("=" * 80)
    
    if len(crawler.chapters) > 0:
        test_chapter = crawler.chapters[0]
        logger.info(f"\n   æµ‹è¯•ç« èŠ‚: {test_chapter['title']}")
        logger.info(f"   ç« èŠ‚URL: {test_chapter['url']}")
        
        content = crawler.download_chapter_content(test_chapter['url'])
        
        logger.info("\n" + "-" * 80)
        logger.info("âœ… ç« èŠ‚å†…å®¹ä¸‹è½½ç»“æœ:")
        logger.info("-" * 80)
        logger.info(f"   ğŸ“„ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        logger.info(f"   ğŸ“„ å†…å®¹é¢„è§ˆ (å‰200å­—):")
        logger.info(f"   {content[:200]}...")
        
        if len(content) > 0:
            logger.info(f"\n   âœ… ç« èŠ‚å†…å®¹ä¸‹è½½æˆåŠŸ")
        else:
            logger.warning(f"\n   âš ï¸  ç« èŠ‚å†…å®¹ä¸ºç©º")
    
    # æµ‹è¯•æ€»ç»“
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š æµ‹è¯•æ€»ç»“")
    logger.info("=" * 80)
    
    test_results = {
        'å°è¯´ä¿¡æ¯è§£æ': bool(crawler.novel_info.get('title')),
        'ç« èŠ‚åˆ—è¡¨è§£æ': len(crawler.chapters) > 0,
        'ç« èŠ‚URLæå–': all(ch.get('url') for ch in crawler.chapters[:5]),
        'ç« èŠ‚å†…å®¹ä¸‹è½½': len(content) > 0 if len(crawler.chapters) > 0 else False
    }
    
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        logger.info(f"   {test_name}: {status}")
    
    all_passed = all(test_results.values())
    
    if all_passed:
        logger.success("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«åŠŸèƒ½å®Œæ•´ä¸”æ­£å¸¸å·¥ä½œ")
    else:
        logger.error("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç«™ç»“æ„")
    
    logger.info("=" * 80)
    
    return all_passed


def test_post_process_rules():
    """
    æµ‹è¯•åå¤„ç†è§„åˆ™ç³»ç»Ÿ
    éªŒè¯æ‰€æœ‰å­—æ®µéƒ½æ”¯æŒåå¤„ç†è§„åˆ™
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ§ª æµ‹è¯•åå¤„ç†è§„åˆ™ç³»ç»Ÿ")
    logger.info("=" * 80)
    
    from backend.parser import HtmlParser
    
    parser = HtmlParser("https://example.com")
    
    # æµ‹è¯•å„ç§åå¤„ç†æ–¹æ³•
    test_cases = [
        {
            'name': 'strip - å»é™¤ç©ºç™½',
            'data': '  æµ‹è¯•æ–‡æœ¬  ',
            'process': [{'method': 'strip', 'params': {}}],
            'expected': 'æµ‹è¯•æ–‡æœ¬'
        },
        {
            'name': 'replace - å­—ç¬¦ä¸²æ›¿æ¢',
            'data': 'ä½œè€…ï¼šå¼ ä¸‰',
            'process': [{'method': 'replace', 'params': {'old': 'ä½œè€…ï¼š', 'new': ''}}],
            'expected': 'å¼ ä¸‰'
        },
        {
            'name': 'regex_replace - æ­£åˆ™æ›¿æ¢',
            'data': 'ç¬¬123ç«  æµ‹è¯•',
            'process': [{'method': 'regex_replace', 'params': {'pattern': r'ç¬¬\d+ç« \s+', 'repl': ''}}],
            'expected': 'æµ‹è¯•'
        },
        {
            'name': 'join - åˆ—è¡¨è¿æ¥',
            'data': ['è¡Œ1', 'è¡Œ2', 'è¡Œ3'],
            'process': [{'method': 'join', 'params': {'separator': '\n'}}],
            'expected': 'è¡Œ1\nè¡Œ2\nè¡Œ3'
        },
        {
            'name': 'split - å­—ç¬¦ä¸²åˆ†å‰²',
            'data': 'å¼ ä¸‰,æå››,ç‹äº”',
            'process': [{'method': 'split', 'params': {'separator': ','}}],
            'expected': ['å¼ ä¸‰', 'æå››', 'ç‹äº”']
        }
    ]
    
    all_passed = True
    
    for test in test_cases:
        result = parser.apply_post_process(test['data'], test['process'])
        passed = result == test['expected']
        all_passed = all_passed and passed
        
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        logger.info(f"\n   {test['name']}: {status}")
        logger.info(f"      è¾“å…¥: {test['data']}")
        logger.info(f"      è¾“å‡º: {result}")
        logger.info(f"      æœŸæœ›: {test['expected']}")
    
    if all_passed:
        logger.success("\nâœ… æ‰€æœ‰åå¤„ç†è§„åˆ™æµ‹è¯•é€šè¿‡ï¼")
    else:
        logger.error("\nâŒ éƒ¨åˆ†åå¤„ç†è§„åˆ™æµ‹è¯•å¤±è´¥")
    
    return all_passed


if __name__ == '__main__':
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œæµ‹è¯•...")
    logger.info("")
    
    # æµ‹è¯•1: å®Œæ•´å·¥ä½œæµç¨‹
    workflow_passed = test_djks5_workflow()
    
    # æµ‹è¯•2: åå¤„ç†è§„åˆ™
    process_passed = test_post_process_rules()
    
    # æœ€ç»ˆç»“æœ
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ æµ‹è¯•å®Œæˆ")
    logger.info("=" * 80)
    logger.info(f"   å®Œæ•´æµç¨‹æµ‹è¯•: {'âœ… é€šè¿‡' if workflow_passed else 'âŒ å¤±è´¥'}")
    logger.info(f"   åå¤„ç†è§„åˆ™æµ‹è¯•: {'âœ… é€šè¿‡' if process_passed else 'âŒ å¤±è´¥'}")
    
    if workflow_passed and process_passed:
        logger.success("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼ç³»ç»ŸåŠŸèƒ½å®Œæ•´ï¼")
        sys.exit(0)
    else:
        logger.error("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        sys.exit(1)

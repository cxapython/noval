#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•çˆ¬è™«ä»£ç ç”ŸæˆåŠŸèƒ½
"""
import sys
from pathlib import Path
import json
import requests

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from loguru import logger

API_BASE = 'http://localhost:5001/api/crawler'


def test_generate_crawler_code():
    """æµ‹è¯•ç”Ÿæˆçˆ¬è™«ä»£ç """
    logger.info("=" * 60)
    logger.info("æµ‹è¯•ç”Ÿæˆçˆ¬è™«ä»£ç åŠŸèƒ½")
    logger.info("=" * 60)
    
    # 1. é¦–å…ˆè·å–é…ç½®åˆ—è¡¨
    logger.info("1. è·å–é…ç½®åˆ—è¡¨...")
    response = requests.get(f"{API_BASE}/configs")
    
    if not response.json().get('success'):
        logger.error("è·å–é…ç½®åˆ—è¡¨å¤±è´¥")
        return False
    
    configs = response.json().get('configs', [])
    if not configs:
        logger.warning("âš ï¸  æ²¡æœ‰å¯ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œè¯·å…ˆåˆ›å»ºé…ç½®")
        return False
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªé…ç½®è¿›è¡Œæµ‹è¯•
    test_config = configs[0]
    filename = test_config['filename']
    logger.info(f"ä½¿ç”¨é…ç½®: {filename}")
    
    # 2. ç”Ÿæˆçˆ¬è™«ä»£ç 
    logger.info(f"2. ç”Ÿæˆçˆ¬è™«ä»£ç ...")
    response = requests.post(f"{API_BASE}/generate-crawler/{filename}")
    
    if not response.json().get('success'):
        logger.error(f"ç”Ÿæˆä»£ç å¤±è´¥: {response.json().get('error')}")
        return False
    
    result = response.json()
    generated_code = result.get('content')
    generated_filename = result.get('filename')
    
    logger.info(f"âœ… ä»£ç ç”ŸæˆæˆåŠŸï¼")
    logger.info(f"   æ–‡ä»¶å: {generated_filename}")
    logger.info(f"   ä»£ç é•¿åº¦: {len(generated_code)} å­—ç¬¦")
    
    # 3. éªŒè¯ç”Ÿæˆçš„ä»£ç 
    logger.info("3. éªŒè¯ä»£ç å†…å®¹...")
    
    # æ£€æŸ¥å¿…è¦çš„å†…å®¹
    required_elements = [
        'import sys',
        'from pathlib import Path',
        'from backend.generic_crawler import GenericNovelCrawler',
        'def __init__',
        'def run',
        'def main',
        "if __name__ == '__main__':"
    ]
    
    missing_elements = []
    for element in required_elements:
        if element not in generated_code:
            missing_elements.append(element)
    
    if missing_elements:
        logger.error(f"âŒ ä»£ç ç¼ºå°‘å¿…è¦å…ƒç´ : {missing_elements}")
        return False
    
    logger.info("âœ… ä»£ç åŒ…å«æ‰€æœ‰å¿…è¦å…ƒç´ ")
    
    # 4. æ˜¾ç¤ºä»£ç é¢„è§ˆ
    logger.info("4. ä»£ç é¢„è§ˆï¼ˆå‰30è¡Œï¼‰:")
    lines = generated_code.split('\n')
    for i, line in enumerate(lines[:30], 1):
        logger.info(f"   {i:3d} | {line}")
    
    if len(lines) > 30:
        logger.info(f"   ... (è¿˜æœ‰ {len(lines) - 30} è¡Œ)")
    
    # 5. æµ‹è¯•ä¿å­˜ä»£ç 
    logger.info("5. æµ‹è¯•ä¿å­˜ä»£ç ...")
    test_filename = f"test_{generated_filename}"
    
    response = requests.post(f"{API_BASE}/save-crawler", json={
        'filename': test_filename,
        'content': generated_code
    })
    
    if response.json().get('success'):
        logger.info(f"âœ… ä»£ç ä¿å­˜æˆåŠŸ: {test_filename}")
        saved_path = project_root / test_filename
        if saved_path.exists():
            logger.info(f"   æ–‡ä»¶ä½ç½®: {saved_path}")
            # åˆ é™¤æµ‹è¯•æ–‡ä»¶
            saved_path.unlink()
            logger.info(f"   æµ‹è¯•æ–‡ä»¶å·²æ¸…ç†")
    else:
        logger.warning(f"âš ï¸  ä¿å­˜å¤±è´¥: {response.json().get('error')}")
    
    logger.info("=" * 60)
    logger.info("âœ… æµ‹è¯•å®Œæˆï¼")
    logger.info("=" * 60)
    return True


def test_api_endpoints():
    """æµ‹è¯•æ‰€æœ‰ç›¸å…³çš„APIç«¯ç‚¹"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯•APIç«¯ç‚¹")
    logger.info("=" * 60)
    
    endpoints = [
        ('GET', f"{API_BASE}/configs", None),
    ]
    
    for method, url, data in endpoints:
        logger.info(f"æµ‹è¯• {method} {url}")
        try:
            if method == 'GET':
                response = requests.get(url, timeout=5)
            else:
                response = requests.post(url, json=data, timeout=5)
            
            if response.status_code == 200:
                logger.info(f"  âœ… å“åº”æˆåŠŸ (çŠ¶æ€ç : {response.status_code})")
            else:
                logger.warning(f"  âš ï¸  å“åº”å¼‚å¸¸ (çŠ¶æ€ç : {response.status_code})")
        except requests.exceptions.ConnectionError:
            logger.error(f"  âŒ è¿æ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿åç«¯æœåŠ¡æ­£åœ¨è¿è¡Œ")
            return False
        except Exception as e:
            logger.error(f"  âŒ è¯·æ±‚å¤±è´¥: {e}")
            return False
    
    return True


if __name__ == '__main__':
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•çˆ¬è™«ä»£ç ç”ŸæˆåŠŸèƒ½")
    logger.info("")
    
    # æµ‹è¯•APIç«¯ç‚¹
    if not test_api_endpoints():
        logger.error("âŒ APIç«¯ç‚¹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥åç«¯æœåŠ¡")
        sys.exit(1)
    
    logger.info("")
    
    # æµ‹è¯•ç”Ÿæˆä»£ç 
    if test_generate_crawler_code():
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        sys.exit(0)
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥")
        sys.exit(1)

